{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841c26c3-69e3-4750-a7a0-61c17d7da549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (4.46.0.dev0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface_hub in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (0.24.7)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface_hub) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface_hub) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface_hub) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (0.34.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from accelerate>=0.26.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from accelerate>=0.26.0) (24.0)\n",
      "Requirement already satisfied: psutil in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from accelerate>=0.26.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from accelerate>=0.26.0) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from accelerate>=0.26.0) (0.24.7)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.0)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.12.1)\n",
      "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.26.0) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (4.46.0.dev0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install numpy\n",
    "%pip install huggingface_hub\n",
    "%pip install 'accelerate>=0.26.0'\n",
    "%pip install --upgrade transformers\n",
    "\n",
    "from huggingface_hub import login\n",
    "import local_config\n",
    "\n",
    "login(token=local_config.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a01cf86-4bbf-45b5-bc1a-128270b27d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#from transformers import MllamaForConditionalGeneration\n",
    "\n",
    "#original_model = MllamaForConditionalGeneration.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3dd161b-4f41-4b10-a867-53a2ac5f334f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    #print(\"original_model\", original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fdabc5f-2aec-403f-aca0-16730bc52790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:41<00:00,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model MllamaForConditionalGeneration(\n",
      "  (vision_model): MllamaVisionModel(\n",
      "    (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), padding=valid, bias=False)\n",
      "    (gated_positional_embedding): MllamaPrecomputedPositionEmbedding(\n",
      "      (tile_embedding): Embedding(9, 5248000)\n",
      "    )\n",
      "    (pre_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
      "      (embedding): Embedding(9, 5120)\n",
      "    )\n",
      "    (post_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
      "      (embedding): Embedding(9, 5120)\n",
      "    )\n",
      "    (layernorm_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    (layernorm_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): MllamaVisionEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x MllamaVisionEncoderLayer(\n",
      "          (self_attn): MllamaVisionSdpaAttention(\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaVisionMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "          (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (global_transformer): MllamaVisionEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-7): 8 x MllamaVisionEncoderLayer(\n",
      "          (self_attn): MllamaVisionSdpaAttention(\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaVisionMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "          (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (language_model): MllamaForCausalLM(\n",
      "    (model): MllamaTextModel(\n",
      "      (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (3): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (8): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (13): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (18): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (23): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (28): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (33): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (38): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (39): MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "      (rotary_emb): MllamaRotaryEmbedding()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "  )\n",
      "  (multi_modal_projector): Linear(in_features=7680, out_features=4096, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import MllamaForConditionalGeneration\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "\n",
    "pretrained_model = MllamaForConditionalGeneration.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\")\n",
    "pretrained_config = pretrained_model.config\n",
    "\n",
    "print(\"pretrained_model\", pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b717751d-3380-4256-8313-4d64851046c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00,  7.67it/s]\n",
      "Some weights of MllamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-11B-Vision and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.cross_attn.k_norm.weight', 'layers.13.cross_attn.k_proj.weight', 'layers.13.cross_attn.o_proj.weight', 'layers.13.cross_attn.q_norm.weight', 'layers.13.cross_attn.q_proj.weight', 'layers.13.cross_attn.v_proj.weight', 'layers.13.cross_attn_attn_gate', 'layers.13.cross_attn_mlp_gate', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.cross_attn.k_norm.weight', 'layers.18.cross_attn.k_proj.weight', 'layers.18.cross_attn.o_proj.weight', 'layers.18.cross_attn.q_norm.weight', 'layers.18.cross_attn.q_proj.weight', 'layers.18.cross_attn.v_proj.weight', 'layers.18.cross_attn_attn_gate', 'layers.18.cross_attn_mlp_gate', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.cross_attn.k_norm.weight', 'layers.23.cross_attn.k_proj.weight', 'layers.23.cross_attn.o_proj.weight', 'layers.23.cross_attn.q_norm.weight', 'layers.23.cross_attn.q_proj.weight', 'layers.23.cross_attn.v_proj.weight', 'layers.23.cross_attn_attn_gate', 'layers.23.cross_attn_mlp_gate', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.28.cross_attn.k_norm.weight', 'layers.28.cross_attn.k_proj.weight', 'layers.28.cross_attn.o_proj.weight', 'layers.28.cross_attn.q_norm.weight', 'layers.28.cross_attn.q_proj.weight', 'layers.28.cross_attn.v_proj.weight', 'layers.28.cross_attn_attn_gate', 'layers.28.cross_attn_mlp_gate', 'layers.28.input_layernorm.weight', 'layers.28.mlp.down_proj.weight', 'layers.28.mlp.gate_proj.weight', 'layers.28.mlp.up_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.29.input_layernorm.weight', 'layers.29.mlp.down_proj.weight', 'layers.29.mlp.gate_proj.weight', 'layers.29.mlp.up_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.29.self_attn.k_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.29.self_attn.v_proj.weight', 'layers.3.cross_attn.k_norm.weight', 'layers.3.cross_attn.k_proj.weight', 'layers.3.cross_attn.o_proj.weight', 'layers.3.cross_attn.q_norm.weight', 'layers.3.cross_attn.q_proj.weight', 'layers.3.cross_attn.v_proj.weight', 'layers.3.cross_attn_attn_gate', 'layers.3.cross_attn_mlp_gate', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.30.input_layernorm.weight', 'layers.30.mlp.down_proj.weight', 'layers.30.mlp.gate_proj.weight', 'layers.30.mlp.up_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.30.self_attn.k_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.31.input_layernorm.weight', 'layers.31.mlp.down_proj.weight', 'layers.31.mlp.gate_proj.weight', 'layers.31.mlp.up_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.31.self_attn.k_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.32.input_layernorm.weight', 'layers.32.mlp.down_proj.weight', 'layers.32.mlp.gate_proj.weight', 'layers.32.mlp.up_proj.weight', 'layers.32.post_attention_layernorm.weight', 'layers.32.self_attn.k_proj.weight', 'layers.32.self_attn.o_proj.weight', 'layers.32.self_attn.q_proj.weight', 'layers.32.self_attn.v_proj.weight', 'layers.33.cross_attn.k_norm.weight', 'layers.33.cross_attn.k_proj.weight', 'layers.33.cross_attn.o_proj.weight', 'layers.33.cross_attn.q_norm.weight', 'layers.33.cross_attn.q_proj.weight', 'layers.33.cross_attn.v_proj.weight', 'layers.33.cross_attn_attn_gate', 'layers.33.cross_attn_mlp_gate', 'layers.33.input_layernorm.weight', 'layers.33.mlp.down_proj.weight', 'layers.33.mlp.gate_proj.weight', 'layers.33.mlp.up_proj.weight', 'layers.33.post_attention_layernorm.weight', 'layers.34.input_layernorm.weight', 'layers.34.mlp.down_proj.weight', 'layers.34.mlp.gate_proj.weight', 'layers.34.mlp.up_proj.weight', 'layers.34.post_attention_layernorm.weight', 'layers.34.self_attn.k_proj.weight', 'layers.34.self_attn.o_proj.weight', 'layers.34.self_attn.q_proj.weight', 'layers.34.self_attn.v_proj.weight', 'layers.35.input_layernorm.weight', 'layers.35.mlp.down_proj.weight', 'layers.35.mlp.gate_proj.weight', 'layers.35.mlp.up_proj.weight', 'layers.35.post_attention_layernorm.weight', 'layers.35.self_attn.k_proj.weight', 'layers.35.self_attn.o_proj.weight', 'layers.35.self_attn.q_proj.weight', 'layers.35.self_attn.v_proj.weight', 'layers.36.input_layernorm.weight', 'layers.36.mlp.down_proj.weight', 'layers.36.mlp.gate_proj.weight', 'layers.36.mlp.up_proj.weight', 'layers.36.post_attention_layernorm.weight', 'layers.36.self_attn.k_proj.weight', 'layers.36.self_attn.o_proj.weight', 'layers.36.self_attn.q_proj.weight', 'layers.36.self_attn.v_proj.weight', 'layers.37.input_layernorm.weight', 'layers.37.mlp.down_proj.weight', 'layers.37.mlp.gate_proj.weight', 'layers.37.mlp.up_proj.weight', 'layers.37.post_attention_layernorm.weight', 'layers.37.self_attn.k_proj.weight', 'layers.37.self_attn.o_proj.weight', 'layers.37.self_attn.q_proj.weight', 'layers.37.self_attn.v_proj.weight', 'layers.38.cross_attn.k_norm.weight', 'layers.38.cross_attn.k_proj.weight', 'layers.38.cross_attn.o_proj.weight', 'layers.38.cross_attn.q_norm.weight', 'layers.38.cross_attn.q_proj.weight', 'layers.38.cross_attn.v_proj.weight', 'layers.38.cross_attn_attn_gate', 'layers.38.cross_attn_mlp_gate', 'layers.38.input_layernorm.weight', 'layers.38.mlp.down_proj.weight', 'layers.38.mlp.gate_proj.weight', 'layers.38.mlp.up_proj.weight', 'layers.38.post_attention_layernorm.weight', 'layers.39.input_layernorm.weight', 'layers.39.mlp.down_proj.weight', 'layers.39.mlp.gate_proj.weight', 'layers.39.mlp.up_proj.weight', 'layers.39.post_attention_layernorm.weight', 'layers.39.self_attn.k_proj.weight', 'layers.39.self_attn.o_proj.weight', 'layers.39.self_attn.q_proj.weight', 'layers.39.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.cross_attn.k_norm.weight', 'layers.8.cross_attn.k_proj.weight', 'layers.8.cross_attn.o_proj.weight', 'layers.8.cross_attn.q_norm.weight', 'layers.8.cross_attn.q_proj.weight', 'layers.8.cross_attn.v_proj.weight', 'layers.8.cross_attn_attn_gate', 'layers.8.cross_attn_mlp_gate', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'lm_head.weight', 'norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No vision model found in pretrained model. Initializing vision models randomly.\n",
      "Parameter model.embed_tokens.weight not found in new model. Skipping.\n",
      "Parameter model.layers.0.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.0.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.0.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.0.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.0.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.0.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.0.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.0.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.0.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.1.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.1.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.1.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.1.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.1.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.1.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.1.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.1.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.1.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.2.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.2.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.2.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.2.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.2.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.2.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.2.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.2.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.2.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.cross_attn_attn_gate not found in new model. Skipping.\n",
      "Parameter model.layers.3.cross_attn_mlp_gate not found in new model. Skipping.\n",
      "Parameter model.layers.3.cross_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.cross_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.cross_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.cross_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.cross_attn.q_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.cross_attn.k_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.3.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.4.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.4.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.4.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.4.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.4.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.4.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.4.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.4.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.4.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.5.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.5.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.5.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.5.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.5.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.5.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.5.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.5.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.5.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.6.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.6.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.6.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.6.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.6.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.6.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.6.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.6.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.6.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.7.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.7.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.7.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.7.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.7.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.7.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.7.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.7.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.7.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.cross_attn_attn_gate not found in new model. Skipping.\n",
      "Parameter model.layers.8.cross_attn_mlp_gate not found in new model. Skipping.\n",
      "Parameter model.layers.8.cross_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.cross_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.cross_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.cross_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.cross_attn.q_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.cross_attn.k_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.8.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.9.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.9.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.9.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.9.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.9.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.9.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.9.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.9.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.9.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.10.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.10.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.10.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.10.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.10.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.10.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.10.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.10.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.10.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.11.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.11.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.11.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.11.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.11.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.11.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.11.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.11.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.11.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.12.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.12.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.12.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.12.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.12.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.12.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.12.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.12.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.12.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.cross_attn_attn_gate not found in new model. Skipping.\n",
      "Parameter model.layers.13.cross_attn_mlp_gate not found in new model. Skipping.\n",
      "Parameter model.layers.13.cross_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.cross_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.cross_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.cross_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.cross_attn.q_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.cross_attn.k_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.13.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.14.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.14.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.14.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.14.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.14.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.14.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.14.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.14.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.14.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.15.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.15.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.15.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.15.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.15.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.15.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.15.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.15.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.15.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.16.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.16.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.16.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.16.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.16.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.16.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.16.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.16.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.16.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.17.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.17.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.17.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.17.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.17.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.17.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.17.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.17.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.17.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.cross_attn_attn_gate not found in new model. Skipping.\n",
      "Parameter model.layers.18.cross_attn_mlp_gate not found in new model. Skipping.\n",
      "Parameter model.layers.18.cross_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.cross_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.cross_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.cross_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.cross_attn.q_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.cross_attn.k_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.18.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.19.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.19.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.19.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.19.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.19.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.19.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.19.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.19.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.19.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.20.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.20.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.20.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.20.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.20.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.20.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.20.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.20.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.20.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.21.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.21.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.21.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.21.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.21.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.21.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.21.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.21.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.21.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.22.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.22.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.22.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.22.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.22.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.22.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.22.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.22.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.22.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.cross_attn_attn_gate not found in new model. Skipping.\n",
      "Parameter model.layers.23.cross_attn_mlp_gate not found in new model. Skipping.\n",
      "Parameter model.layers.23.cross_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.cross_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.cross_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.cross_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.cross_attn.q_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.cross_attn.k_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.23.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.24.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.24.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.24.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.24.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.24.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.24.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.24.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.24.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.24.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.25.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.25.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.25.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.25.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.25.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.25.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.25.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.25.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.25.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.26.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.26.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.26.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.26.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.26.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.26.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.26.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.26.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.26.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.27.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.27.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.27.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.27.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.27.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.27.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.27.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.27.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.27.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.cross_attn_attn_gate not found in new model. Skipping.\n",
      "Parameter model.layers.28.cross_attn_mlp_gate not found in new model. Skipping.\n",
      "Parameter model.layers.28.cross_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.cross_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.cross_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.cross_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.cross_attn.q_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.cross_attn.k_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.28.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.29.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.29.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.29.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.29.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.29.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.29.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.29.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.29.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.29.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.30.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.30.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.30.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.30.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.30.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.30.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.30.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.30.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.30.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.31.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.31.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.31.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.31.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.31.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.31.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.31.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.31.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.31.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.32.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.32.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.32.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.32.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.32.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.32.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.32.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.32.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.32.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.cross_attn_attn_gate not found in new model. Skipping.\n",
      "Parameter model.layers.33.cross_attn_mlp_gate not found in new model. Skipping.\n",
      "Parameter model.layers.33.cross_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.cross_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.cross_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.cross_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.cross_attn.q_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.cross_attn.k_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.33.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.34.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.34.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.34.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.34.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.34.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.34.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.34.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.34.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.34.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.35.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.35.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.35.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.35.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.35.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.35.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.35.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.35.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.35.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.36.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.36.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.36.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.36.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.36.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.36.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.36.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.36.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.36.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.37.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.37.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.37.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.37.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.37.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.37.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.37.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.37.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.37.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.cross_attn_attn_gate not found in new model. Skipping.\n",
      "Parameter model.layers.38.cross_attn_mlp_gate not found in new model. Skipping.\n",
      "Parameter model.layers.38.cross_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.cross_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.cross_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.cross_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.cross_attn.q_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.cross_attn.k_norm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.38.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.39.self_attn.q_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.39.self_attn.k_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.39.self_attn.v_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.39.self_attn.o_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.39.mlp.gate_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.39.mlp.up_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.39.mlp.down_proj.weight not found in new model. Skipping.\n",
      "Parameter model.layers.39.input_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.layers.39.post_attention_layernorm.weight not found in new model. Skipping.\n",
      "Parameter model.norm.weight not found in new model. Skipping.\n",
      "Parameter lm_head.weight not found in new model. Skipping.\n",
      "No multi_modal_projector found in pretrained model. Initializing randomly.\n",
      "Weights loaded into the modified model.\n",
      "Modified model with loaded weights:\n",
      "MllamaForConditionalGeneration(\n",
      "  (vision_model_1): MllamaVisionModel(\n",
      "    (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
      "    (transformer): ModuleList(\n",
      "      (0-31): 32 x MllamaVisionEncoderLayer(\n",
      "        (attention): MllamaVisionSdpaAttention(\n",
      "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (mlp): MllamaVisionMLP(\n",
      "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          (act_fn): GELU(approximate='none')\n",
      "        )\n",
      "        (layernorm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_model_2): MllamaVisionModel(\n",
      "    (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
      "    (transformer): ModuleList(\n",
      "      (0-31): 32 x MllamaVisionEncoderLayer(\n",
      "        (attention): MllamaVisionSdpaAttention(\n",
      "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (mlp): MllamaVisionMLP(\n",
      "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          (act_fn): GELU(approximate='none')\n",
      "        )\n",
      "        (layernorm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_cross_attention): MllamaVisionCrossAttention(\n",
      "    (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (language_model): MllamaForCausalLM(\n",
      "    (transformer): MllamaTextModel(\n",
      "      (embed_tokens): Embedding(128256, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-39): 40 x MllamaDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm()\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm()\n",
      "        )\n",
      "      )\n",
      "      (norm): MllamaTextRMSNorm()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "  )\n",
      "  (multi_modal_projector): Linear(in_features=2560, out_features=4096, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 378\u001b[0m\n\u001b[1;32m    375\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m create_attention_mask_from_input_ids(input_ids)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(image_1, image_2, input_ids, attention_mask)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Process outputs (e.g., get predicted token)\u001b[39;00m\n\u001b[1;32m    381\u001b[0m predicted_token \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "class MllamaForConditionalGeneration(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_model_1 = MllamaVisionModel(config)\n",
    "        self.vision_model_2 = MllamaVisionModel(config)\n",
    "        self.vision_cross_attention = MllamaVisionCrossAttention(config)\n",
    "        self.language_model = MllamaForCausalLM(config)\n",
    "        self.multi_modal_projector = nn.Linear(config.vision_hidden_size * 2, config.hidden_size)\n",
    "\n",
    "    def forward(self, image_1, image_2, input_ids, attention_mask=None):\n",
    "        vision_output_1 = self.vision_model_1(image_1)\n",
    "        vision_output_2 = self.vision_model_2(image_2)\n",
    "        \n",
    "        # Cross-attention between vision models\n",
    "        vision_output_1 = self.vision_cross_attention(vision_output_1, vision_output_2)\n",
    "        vision_output_2 = self.vision_cross_attention(vision_output_2, vision_output_1)\n",
    "        \n",
    "        # Concatenate outputs from both vision models\n",
    "        combined_vision_output = torch.cat([vision_output_1, vision_output_2], dim=-1)\n",
    "        \n",
    "        # Project combined vision output to language model dimension\n",
    "        projected_vision_output = self.multi_modal_projector(combined_vision_output)\n",
    "        \n",
    "        # Combine with text input and pass through language model\n",
    "        lm_output = self.language_model(input_ids, attention_mask, projected_vision_output)\n",
    "        \n",
    "        return lm_output\n",
    "\n",
    "class MllamaVisionCrossAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(config.vision_hidden_size, config.vision_hidden_size)\n",
    "        self.key = nn.Linear(config.vision_hidden_size, config.vision_hidden_size)\n",
    "        self.value = nn.Linear(config.vision_hidden_size, config.vision_hidden_size)\n",
    "        self.num_attention_heads = config.num_vision_attention_heads\n",
    "        self.attention_head_size = config.vision_hidden_size // config.num_vision_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "        \n",
    "    def forward(self, hidden_states, context):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(context)\n",
    "        mixed_value_layer = self.value(context)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "class MllamaVisionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embedding = nn.Conv2d(3, config.vision_hidden_size, kernel_size=config.patch_size, stride=config.patch_size)\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, (config.image_size // config.patch_size) ** 2, config.vision_hidden_size))\n",
    "        self.transformer = nn.ModuleList([MllamaVisionEncoderLayer(config) for _ in range(config.num_vision_layers)])\n",
    "        self.layernorm = nn.LayerNorm(config.vision_hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        x = self.patch_embedding(pixel_values).flatten(2).transpose(1, 2)\n",
    "        x = x + self.positional_embedding\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class MllamaVisionEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MllamaVisionSdpaAttention(config)\n",
    "        self.mlp = MllamaVisionMLP(config)\n",
    "        self.layernorm1 = nn.LayerNorm(config.vision_hidden_size)\n",
    "        self.layernorm2 = nn.LayerNorm(config.vision_hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attention_output = self.attention(self.layernorm1(hidden_states))\n",
    "        hidden_states = hidden_states + attention_output\n",
    "        mlp_output = self.mlp(self.layernorm2(hidden_states))\n",
    "        hidden_states = hidden_states + mlp_output\n",
    "        return hidden_states\n",
    "\n",
    "class MllamaVisionSdpaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_vision_attention_heads\n",
    "        self.attention_head_size = config.vision_hidden_size // config.num_vision_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.vision_hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.vision_hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.vision_hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out_proj = nn.Linear(config.vision_hidden_size, config.vision_hidden_size)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out_proj(context_layer)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "class MllamaVisionMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.vision_hidden_size, config.vision_intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.vision_intermediate_size, config.vision_hidden_size)\n",
    "        self.act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.act_fn(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class MllamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = MllamaTextModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, vision_input=None):\n",
    "        transformer_outputs = self.transformer(input_ids, attention_mask, vision_input)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        return lm_logits\n",
    "\n",
    "class MllamaTextModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([MllamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = MllamaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, vision_input=None):\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "        \n",
    "        if vision_input is not None:\n",
    "            # Concatenate vision input with text embeddings\n",
    "            hidden_states = torch.cat([vision_input, hidden_states], dim=1)\n",
    "            \n",
    "            if attention_mask is not None:\n",
    "                # Create attention mask for vision input (allowing attending to all vision tokens)\n",
    "                vision_attention_mask = torch.ones(\n",
    "                    (vision_input.size(0), 1, 1, vision_input.size(1)),\n",
    "                    dtype=attention_mask.dtype,\n",
    "                    device=attention_mask.device\n",
    "                )\n",
    "                # Concatenate vision attention mask with text attention mask\n",
    "                attention_mask = torch.cat([vision_attention_mask, attention_mask], dim=-1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return (hidden_states,)\n",
    "\n",
    "class MllamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = MllamaTextSelfSdpaAttention(config)\n",
    "        self.mlp = MllamaTextMLP(config)\n",
    "        self.input_layernorm = MllamaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = MllamaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states = self.self_attn(hidden_states, attention_mask=attention_mask)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class MllamaTextSelfSdpaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_length, _ = hidden_states.size()\n",
    "\n",
    "        q = self.q_proj(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context_layer = torch.matmul(attn_weights, v)\n",
    "\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
    "        attn_output = self.o_proj(context_layer)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "class MllamaTextMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class MllamaTextRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.epsilon)\n",
    "        return self.weight * hidden_states.to(self.weight.dtype)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MllamaConfig:\n",
    "    # Vision model config\n",
    "    image_size: int = 224\n",
    "    patch_size: int = 14\n",
    "    num_channels: int = 3\n",
    "    vision_hidden_size: int = 1280\n",
    "    vision_intermediate_size: int = 5120\n",
    "    num_vision_attention_heads: int = 16\n",
    "    num_vision_layers: int = 32\n",
    "\n",
    "    # Language model config\n",
    "    vocab_size: int = 128264\n",
    "    hidden_size: int = 4096\n",
    "    intermediate_size: int = 14336\n",
    "    num_attention_heads: int = 32\n",
    "    num_hidden_layers: int = 40\n",
    "    max_position_embeddings: int = 2048\n",
    "\n",
    "    # Cross-model config\n",
    "    num_cross_attention_layers: int = 8\n",
    "\n",
    "    # Other\n",
    "    rms_norm_eps: float = 1e-6\n",
    "    initializer_range: float = 0.02\n",
    "\n",
    "# Utility functions\n",
    "def create_attention_mask_from_input_ids(input_ids, padding_idx=1):\n",
    "    \"\"\"Create attention mask from input_ids.\"\"\"\n",
    "    attention_mask = (input_ids != padding_idx).long()\n",
    "    attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "    attention_mask = (1.0 - attention_mask) * torch.finfo(torch.float32).min\n",
    "    return attention_mask\n",
    "\n",
    "def create_position_ids_from_input_ids(input_ids, padding_idx=1):\n",
    "    \"\"\"Create position IDs from input IDs.\"\"\"\n",
    "    mask = input_ids.ne(padding_idx).long()\n",
    "    incremental_indices = torch.cumsum(mask, dim=1).long() * mask\n",
    "    return incremental_indices.long() + padding_idx\n",
    "\n",
    "def load_weights_into_modified_model(config):\n",
    "    # Load the pretrained model\n",
    "    pretrained_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\")\n",
    "    pretrained_config = pretrained_model.config\n",
    "\n",
    "    # Update our config with values from the pretrained config\n",
    "    for key, value in pretrained_config.to_dict().items():\n",
    "        if hasattr(config, key):\n",
    "            setattr(config, key, value)\n",
    "\n",
    "    # Initialize our modified model\n",
    "    modified_model = MllamaForConditionalGeneration(config)\n",
    "\n",
    "    # Helper function to set weights with size check\n",
    "    def set_weight(new_module, old_module, prefix=''):\n",
    "        for name, param in old_module.named_parameters():\n",
    "            full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "            if hasattr(new_module, full_name):\n",
    "                new_param = getattr(new_module, full_name)\n",
    "                if isinstance(new_param, torch.nn.Parameter) and param.size() == new_param.size():\n",
    "                    new_param.data.copy_(param.data)\n",
    "                else:\n",
    "                    print(f\"Size mismatch for {full_name}. Skipping.\")\n",
    "            else:\n",
    "                print(f\"Parameter {full_name} not found in new model. Skipping.\")\n",
    "\n",
    "    # Load weights for vision_model_1 and vision_model_2\n",
    "    if hasattr(pretrained_model, 'vision_model'):\n",
    "        set_weight(modified_model.vision_model_1, pretrained_model.vision_model)\n",
    "        set_weight(modified_model.vision_model_2, pretrained_model.vision_model)\n",
    "    else:\n",
    "        print(\"No vision model found in pretrained model. Initializing vision models randomly.\")\n",
    "\n",
    "    # Load weights for language_model\n",
    "    set_weight(modified_model.language_model, pretrained_model)\n",
    "\n",
    "    # Load weights for multi_modal_projector\n",
    "    if hasattr(pretrained_model, 'multi_modal_projector'):\n",
    "        old_projector = pretrained_model.multi_modal_projector\n",
    "        new_projector = modified_model.multi_modal_projector\n",
    "        if old_projector.weight.size(0) == new_projector.weight.size(0):\n",
    "            # If output size matches, duplicate the input connections\n",
    "            new_projector.weight.data = torch.cat([old_projector.weight.data, old_projector.weight.data], dim=1)\n",
    "            if hasattr(new_projector, 'bias') and new_projector.bias is not None:\n",
    "                new_projector.bias.data.copy_(old_projector.bias.data)\n",
    "        else:\n",
    "            print(f\"Size mismatch for multi_modal_projector. Initializing randomly.\")\n",
    "    else:\n",
    "        print(\"No multi_modal_projector found in pretrained model. Initializing randomly.\")\n",
    "\n",
    "    # Initialize weights for vision_cross_attention randomly\n",
    "    # (This is a new component, so we keep the random initialization)\n",
    "\n",
    "    print(\"Weights loaded into the modified model.\")\n",
    "    return modified_model\n",
    "\n",
    "\n",
    "\n",
    "config = MllamaConfig()\n",
    "\n",
    "# Load weights into our modified model\n",
    "modified_model = load_weights_into_modified_model(config)\n",
    "\n",
    "print(\"Modified model with loaded weights:\")\n",
    "print(modified_model)\n",
    "\n",
    "# Example input (you would need to replace these with actual data)\n",
    "batch_size = 1\n",
    "seq_length = 20\n",
    "image_1 = torch.randn(batch_size, config.num_channels, config.image_size, config.image_size)\n",
    "image_2 = torch.randn(batch_size, config.num_channels, config.image_size, config.image_size)\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))\n",
    "attention_mask = create_attention_mask_from_input_ids(input_ids)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(image_1, image_2, input_ids, attention_mask)\n",
    "\n",
    "# Process outputs (e.g., get predicted token)\n",
    "predicted_token = outputs.argmax(dim=-1)\n",
    "\n",
    "print(f\"Shape of model output: {outputs.shape}\")\n",
    "print(f\"Predicted token: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e1907-8988-4f2d-b60d-7196d83e9d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
