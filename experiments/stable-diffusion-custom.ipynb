{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1495edef-3e31-4f51-bf8b-fad59046019b",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (2.4.1)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (4.46.0.dev0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.23.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: huggingface_hub in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (0.25.2)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.0.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (24.1)\n",
            "Requirement already satisfied: psutil in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.0)\n",
            "Requirement already satisfied: pyyaml in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (2.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.25.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.9.0)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.26.0) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (4.46.0.dev0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "%pip install torch\n",
        "%pip install transformers\n",
        "%pip install numpy\n",
        "%pip install huggingface_hub\n",
        "%pip install 'accelerate>=0.26.0'\n",
        "%pip install --upgrade transformers\n",
        "\n",
        "from huggingface_hub import login\n",
        "import local_config\n",
        "\n",
        "login(token=local_config.TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0569eb12-827f-4055-aa91-6d5e0793c3b4",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: IPython in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (8.18.0)\n",
            "Requirement already satisfied: decorator in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from IPython) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from IPython) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from IPython) (3.0.30)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from IPython) (2.18.0)\n",
            "Requirement already satisfied: stack-data in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from IPython) (0.6.2)\n",
            "Requirement already satisfied: traitlets>=5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from IPython) (5.14.3)\n",
            "Requirement already satisfied: exceptiongroup in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from IPython) (1.2.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from stack-data->IPython) (2.1.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from stack-data->IPython) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from stack-data->IPython) (0.2.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->IPython) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install IPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6886bb3c-b9c1-4b9b-a89a-3472bf9aff3c",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (0.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9bb479a7-341e-4448-b2a0-536e2593f242",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: diffusers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (0.30.3)\n",
            "Requirement already satisfied: importlib-metadata in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (0.25.2)\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (2024.8.30)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "895c185e-44b0-4330-9998-cc2fb1afd3e8",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (4.46.0.dev0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9d66e50e-8b3a-43c6-a4ef-e93346474a07",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'import torch\\nfrom transformers import LlamaModel, LlamaConfig, MllamaProcessor, MllamaVisionModel\\nfrom transformers import AutoProcessor, AutoModelForPreTraining\\nimport numpy as np\\nimport os\\nfrom huggingface_hub import HfApi, HfFolder\\nfrom huggingface_hub.utils import RepositoryNotFoundError, RevisionNotFoundError\\n\\n\\nHfFolder.save_token(local_config.TOKEN)\\n\\n# Create a directory to save the model\\n#os.makedirs(\"downloaded_model\", exist_ok=True)\\n\\nprocessor = MllamaProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\", token=local_config.TOKEN)\\n#model = MllamaVisionModel.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\", token=local_config.TOKEN)\\n\\ncheckpoint = \"meta-llama/Llama-3.2-11B-Vision\"\\nmodel = MllamaVisionModel.from_pretrained(checkpoint, token=local_config.TOKEN)\\nprint(\"model\", model)'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"import torch\n",
        "from transformers import LlamaModel, LlamaConfig, MllamaProcessor, MllamaVisionModel\n",
        "from transformers import AutoProcessor, AutoModelForPreTraining\n",
        "import numpy as np\n",
        "import os\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "from huggingface_hub.utils import RepositoryNotFoundError, RevisionNotFoundError\n",
        "\n",
        "\n",
        "HfFolder.save_token(local_config.TOKEN)\n",
        "\n",
        "# Create a directory to save the model\n",
        "#os.makedirs(\"downloaded_model\", exist_ok=True)\n",
        "\n",
        "processor = MllamaProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\", token=local_config.TOKEN)\n",
        "#model = MllamaVisionModel.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\", token=local_config.TOKEN)\n",
        "\n",
        "checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
        "model = MllamaVisionModel.from_pretrained(checkpoint, token=local_config.TOKEN)\n",
        "print(\"model\", model)\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "87e928a2-255a-4a15-9abc-5bacf8933faa",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-j9eg8kwy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-j9eg8kwy\n",
            "  Resolved https://github.com/huggingface/transformers to commit fa3f2db5c7405a742fcb8f686d3754f70db00977\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7460126-0076-45f7-917e-9ebb7b57de79",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3ec8cde2-84d5-48c1-8c60-773b6c614cfe",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.46.0.dev0\n",
            "4.46.0.dev0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-14 13:56:14.765970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-14 13:56:14.785983: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-14 13:56:14.792236: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-14 13:56:14.807458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-14 13:56:15.993711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import transformers\n",
        "\n",
        "print(transformers.__version__)\n",
        "\n",
        "import torch\n",
        "print(transformers.__version__)\n",
        "from transformers import MllamaForConditionalGeneration\n",
        "from transformers import LlamaModel, MllamaConfig, MllamaProcessor, MllamaVisionModel\n",
        "from transformers import AutoProcessor, AutoModelForPreTraining, AutoConfig\n",
        "import numpy as np\n",
        "import os\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "from huggingface_hub.utils import RepositoryNotFoundError, RevisionNotFoundError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fff79626-c8f5-4eb4-bcf0-08b7f1262a43",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional, Union, Tuple\n",
        "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import MllamaVisionModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "class CustomMllamaForConditionalGeneration(MllamaForConditionalGeneration):\n",
        "    def __init__(self, config: MllamaConfig):\n",
        "        super().__init__(config)\n",
        "        self.vocab_size = config.text_config.vocab_size\n",
        "        self.hidden_size = config.text_config.hidden_size\n",
        "        self.max_num_tiles = config.vision_config.max_num_tiles\n",
        "        self.vision_output_dim = config.vision_config.vision_output_dim\n",
        "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
        "\n",
        "        self.vision_model = MllamaVisionModel._from_config(\n",
        "            config.vision_config, attn_implementation=config._attn_implementation\n",
        "        )\n",
        "        self.vision_model2 = MllamaVisionModel._from_config(\n",
        "            config.vision_config, attn_implementation=config._attn_implementation\n",
        "        )\n",
        "        \n",
        "        self.vision_model2.transformer = self.vision_model.transformer\n",
        "        self.vision_model2.global_transformer = self.vision_model.global_transformer\n",
        "        self.vision_model2.patch_embedding = self.vision_model.patch_embedding\n",
        "        self.vision_model2.gated_positional_embedding = self.vision_model.gated_positional_embedding\n",
        "        self.vision_model2.pre_tile_positional_embedding = self.vision_model.pre_tile_positional_embedding\n",
        "        self.vision_model2.post_tile_positional_embedding = self.vision_model.post_tile_positional_embedding\n",
        "        self.vision_model2.layernorm_pre = self.vision_model.layernorm_pre\n",
        "        self.vision_model2.layernorm_post = self.vision_model.layernorm_post\n",
        "        self.vision_model2.class_embedding = self.vision_model.class_embedding\n",
        "\n",
        "        self.language_model = MllamaForCausalLM._from_config(\n",
        "            config.text_config, attn_implementation=config._attn_implementation\n",
        "        )\n",
        "        self.multi_modal_projector = nn.Linear(\n",
        "            config.vision_config.vision_output_dim,\n",
        "            config.text_config.hidden_size,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.post_init()\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        pixel_values: Optional[torch.FloatTensor] = None,\n",
        "        aspect_ratio_mask: Optional[torch.Tensor] = None,\n",
        "        aspect_ratio_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attention_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attention_states: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        num_logits_to_keep: int = 0,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
        "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
        "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
        "\n",
        "            num_logits_to_keep (`int`, *optional*):\n",
        "                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n",
        "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
        "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```python\n",
        "        >>> from PIL import Image\n",
        "        >>> import requests\n",
        "        >>> from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
        "\n",
        "        >>> checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
        "        >>> model = MllamaForConditionalGeneration.from_pretrained(checkpoint)\n",
        "        >>> processor = AutoProcessor.from_pretrained(checkpoint)\n",
        "\n",
        "        >>> prompt = \"<|image|>If I had to write a haiku for this one\"\n",
        "        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
        "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "        >>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
        "\n",
        "        >>> # Generate\n",
        "        >>> output = model.generate(**inputs, max_new_tokens=15)\n",
        "\n",
        "        >>> prompt_len = inputs.input_ids.shape[-1]\n",
        "        >>> generated_ids = output[:, prompt_len:]\n",
        "        >>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "        >>> print(generated_text)\n",
        "        [', it would be:.\\\\nA stop sign in Chinatown.\\\\n']\n",
        "        ```\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
        "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
        "\n",
        "        if pixel_values is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\n",
        "                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n",
        "            )\n",
        "\n",
        "        if pixel_values is not None and cross_attention_states is not None:\n",
        "            raise ValueError(\"`pixel_values` and `cross_attention_states` cannot be provided simultaneously\")\n",
        "        \n",
        "        if pixel_values is not None:\n",
        "            #print(\"self.vision_model2.transformer.layers[0].self_attn.q_proj.weight\", self.vision_model2.transformer.layers[0].self_attn.q_proj.weight)\n",
        "            if aspect_ratio_ids is None:\n",
        "                raise ValueError(\"`aspect_ratio_ids` must be provided if `pixel_values` is provided\")\n",
        "            # get vision tokens from vision model\n",
        "            vision_outputs2 = self.vision_model2(\n",
        "                pixel_values=pixel_values,\n",
        "                aspect_ratio_ids=aspect_ratio_ids,\n",
        "                aspect_ratio_mask=aspect_ratio_mask,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                output_attentions=output_attentions,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "            cross_attention_states2 = vision_outputs2[0]\n",
        "            cross_attention_states2 = self.multi_modal_projector(cross_attention_states2).reshape(\n",
        "                -1, cross_attention_states2.shape[-2], self.hidden_size\n",
        "            )\n",
        "            #print(\"cross_attention_states2\", cross_attention_states2)\n",
        "            \n",
        "        if pixel_values is not None:\n",
        "            #print(\"self.vision_model.transformer.layers[0].self_attn.q_proj.weight\", self.vision_model.transformer.layers[0].self_attn.q_proj.weight)\n",
        "            if aspect_ratio_ids is None:\n",
        "                raise ValueError(\"`aspect_ratio_ids` must be provided if `pixel_values` is provided\")\n",
        "            # get vision tokens from vision model\n",
        "            vision_outputs = self.vision_model(\n",
        "                pixel_values=pixel_values,\n",
        "                aspect_ratio_ids=aspect_ratio_ids,\n",
        "                aspect_ratio_mask=aspect_ratio_mask,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                output_attentions=output_attentions,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "            cross_attention_states = vision_outputs[0]\n",
        "            cross_attention_states = self.multi_modal_projector(cross_attention_states).reshape(\n",
        "                -1, cross_attention_states.shape[-2], self.hidden_size\n",
        "            )\n",
        "            #print(\"cross_attention_states\", cross_attention_states)\n",
        "\n",
        "            \n",
        "        \n",
        "        if cross_attention_states is not None:\n",
        "            cross_attention_states = (cross_attention_states + cross_attention_states2) / 2\n",
        "\n",
        "        if cross_attention_mask is not None:\n",
        "            cross_attention_mask, full_text_row_masked_out_mask = _prepare_cross_attention_mask(\n",
        "                cross_attention_mask,\n",
        "                num_vision_tokens=self.vision_model.num_patches,\n",
        "                dtype=self.dtype,\n",
        "            )\n",
        "        else:\n",
        "            full_text_row_masked_out_mask = None\n",
        "\n",
        "        if cross_attention_mask is not None and cache_position is not None:\n",
        "            cross_attention_mask = cross_attention_mask[:, :, cache_position]\n",
        "            full_text_row_masked_out_mask = full_text_row_masked_out_mask[:, :, cache_position]\n",
        "\n",
        "        outputs = self.language_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            cross_attention_states=cross_attention_states,\n",
        "            cross_attention_mask=cross_attention_mask,\n",
        "            full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            labels=labels,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            output_attentions=output_attentions,\n",
        "            return_dict=return_dict,\n",
        "            cache_position=cache_position,\n",
        "            num_logits_to_keep=num_logits_to_keep,\n",
        "        )\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cec1c6ec-375f-49cd-ab36-491eacdcdf5e",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'import torch\\nprint(transformers.__version__)\\nfrom transformers import MllamaForConditionalGeneration\\nfrom transformers import LlamaModel, MllamaConfig, MllamaProcessor, MllamaVisionModel\\nfrom transformers import AutoProcessor, AutoModelForPreTraining, AutoConfig\\nimport numpy as np\\nimport os\\nfrom huggingface_hub import HfApi, HfFolder\\nfrom huggingface_hub.utils import RepositoryNotFoundError, RevisionNotFoundError\\nimport requests\\nimport torch\\nfrom PIL import Image\\nfrom transformers import MllamaForCausalLM, MllamaForConditionalGeneration, AutoProcessor\\n\\n\\nmodel_name = \"meta-llama/Llama-3.2-11B-Vision\"  # You can change this to any model you have access to\\nmodel = CustomMllamaForConditionalGeneration.from_pretrained(model_name, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/wes2/code\", token=local_config.TOKEN)\\n\\n#unaltered_model = MllamaForConditionalGeneration.from_pretrained(model_name, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/wes2/code\", token=local_config.TOKEN)\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"import torch\n",
        "print(transformers.__version__)\n",
        "from transformers import MllamaForConditionalGeneration\n",
        "from transformers import LlamaModel, MllamaConfig, MllamaProcessor, MllamaVisionModel\n",
        "from transformers import AutoProcessor, AutoModelForPreTraining, AutoConfig\n",
        "import numpy as np\n",
        "import os\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "from huggingface_hub.utils import RepositoryNotFoundError, RevisionNotFoundError\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import MllamaForCausalLM, MllamaForConditionalGeneration, AutoProcessor\n",
        "\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-11B-Vision\"  # You can change this to any model you have access to\n",
        "model = CustomMllamaForConditionalGeneration.from_pretrained(model_name, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/wes2/code\", token=local_config.TOKEN)\n",
        "\n",
        "#unaltered_model = MllamaForConditionalGeneration.from_pretrained(model_name, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/wes2/code\", token=local_config.TOKEN)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3b4f1295-7c48-4d4b-bcdc-36e9a2c2dec4",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _prepare_cross_attention_mask(\n",
        "    cross_attention_mask: torch.Tensor,\n",
        "    num_vision_tokens: int,\n",
        "    dtype: str,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    # reshape so it can be used by attn module\n",
        "    batch_size, text_total_length, *_ = cross_attention_mask.shape\n",
        "    cross_attention_mask = cross_attention_mask.repeat_interleave(num_vision_tokens, dim=3)\n",
        "    cross_attention_mask = cross_attention_mask.view(batch_size, text_total_length, -1)\n",
        "    cross_attention_mask = cross_attention_mask.unsqueeze(1)\n",
        "\n",
        "    # invert the mask\n",
        "    inverted_cross_attn_mask = (1.0 - cross_attention_mask).to(dtype)\n",
        "    cross_attention_mask = inverted_cross_attn_mask.masked_fill(\n",
        "        inverted_cross_attn_mask.to(torch.bool), torch.finfo(dtype).min\n",
        "    )\n",
        "\n",
        "    # apply full-row bias, which return 4D tensor of shape [B, H, S1, 1] where value is 0 if the a full row in cross attn mask's\n",
        "    # last dimension contains negative infinity values, otherwise it's 1\n",
        "    negative_inf_value = torch.finfo(dtype).min\n",
        "    full_text_row_masked_out_mask = (\n",
        "        (cross_attention_mask != negative_inf_value).any(dim=-1).type_as(cross_attention_mask)[..., None]\n",
        "    )\n",
        "    cross_attention_mask *= full_text_row_masked_out_mask\n",
        "\n",
        "    return cross_attention_mask, full_text_row_masked_out_mask\n",
        "\n",
        "\n",
        "def _prepare_aspect_ratio_attention_mask(\n",
        "    aspect_ratio_mask: torch.Tensor,\n",
        "    num_patches: int,\n",
        "    target_length: int,\n",
        "    dtype: torch.dtype,\n",
        ") -> torch.Tensor:\n",
        "    # Expand aspect ratio mask to target_length\n",
        "    batch_size, max_num_tiles = aspect_ratio_mask.shape\n",
        "    attention_mask = aspect_ratio_mask.view(batch_size, max_num_tiles, 1, 1).to(dtype)\n",
        "    attention_mask = attention_mask.repeat(1, 1, target_length, 1)\n",
        "\n",
        "    # Mask padding patches\n",
        "    pad_patches = target_length - num_patches\n",
        "    attention_mask[:, :, -pad_patches:] = 0\n",
        "\n",
        "    # Invert the mask (0 -> 1, 1 -> 0)\n",
        "    attention_mask = 1 - attention_mask\n",
        "\n",
        "    # Reshape to 2D and create 4D attention mask\n",
        "    # (batch_size, 1, max_num_tiles * target_length, max_num_tiles * target_length)\n",
        "    attention_mask = attention_mask.reshape(batch_size, max_num_tiles * target_length, 1)\n",
        "    attention_mask = attention_mask @ attention_mask.transpose(-1, -2) * torch.finfo(dtype).min\n",
        "    attention_mask = attention_mask.unsqueeze(1)\n",
        "\n",
        "    return attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4ad38aac-34e7-41c6-8a62-ad521d611f4d",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision\"\\n\\n\\n\\n# Load the original configuration\\noriginal_config = MllamaConfig.from_pretrained(model_name, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/wes2/code\", token=local_config.TOKEN)\\n\\nprocessor = AutoProcessor.from_pretrained(model_id)'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
        "\n",
        "\n",
        "\n",
        "# Load the original configuration\n",
        "original_config = MllamaConfig.from_pretrained(model_name, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/wes2/code\", token=local_config.TOKEN)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3833bf9c-bf97-4433-b1dd-57991f82a330",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\\nurl = \"https://c8.alamy.com/comp/E9J0EW/dog-with-hat-E9J0EW.jpg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nprompt = \"<|image|><|begin_of_text|>If I had to write a haiku for this one\"\\ninputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\\n\\n#output = unaltered_model.generate(**inputs, max_new_tokens=30)\\n#print(processor.decode(output[0]))\\n\\noutput = model.generate(**inputs, max_new_tokens=30)\\nprint(processor.decode(output[0]))'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
        "url = \"https://c8.alamy.com/comp/E9J0EW/dog-with-hat-E9J0EW.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "prompt = \"<|image|><|begin_of_text|>If I had to write a haiku for this one\"\n",
        "inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "#output = unaltered_model.generate(**inputs, max_new_tokens=30)\n",
        "#print(processor.decode(output[0]))\n",
        "\n",
        "output = model.generate(**inputs, max_new_tokens=30)\n",
        "print(processor.decode(output[0]))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "38e6b5e3-14da-4575-8237-829b6c5ef461",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: diffusers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (0.30.3)\n",
            "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (4.46.0.dev0)\n",
            "Requirement already satisfied: scipy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.14.1)\n",
            "Requirement already satisfied: torch in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (2.4.1)\n",
            "Requirement already satisfied: accelerate in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (0.25.2)\n",
            "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (10.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: psutil in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.25.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade diffusers transformers scipy torch accelerate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "e614128e-eb9f-4872-a243-823aa23ae10a",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, Dict, List, Optional, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
        "from diffusers.loaders import FromOriginalModelMixin, PeftAdapterMixin\n",
        "from diffusers.models.attention import JointTransformerBlock\n",
        "from diffusers.models.attention_processor import Attention, AttentionProcessor, FusedJointAttnProcessor2_0\n",
        "from diffusers.models.modeling_utils import ModelMixin\n",
        "from diffusers.models.normalization import AdaLayerNormContinuous\n",
        "from diffusers.utils import USE_PEFT_BACKEND, is_torch_version, logging, scale_lora_layers, unscale_lora_layers\n",
        "from diffusers.models.embeddings import CombinedTimestepTextProjEmbeddings, PatchEmbed\n",
        "from diffusers.models.modeling_outputs import Transformer2DModelOutput\n",
        "from diffusers.models.transformers import SD3Transformer2DModel\n",
        "\n",
        "class CustomSD3Transformer2DModel(SD3Transformer2DModel):\n",
        "    \"\"\"\n",
        "    The Transformer model introduced in Stable Diffusion 3.\n",
        "\n",
        "    Reference: https://arxiv.org/abs/2403.03206\n",
        "\n",
        "    Parameters:\n",
        "        sample_size (`int`): The width of the latent images. This is fixed during training since\n",
        "            it is used to learn a number of position embeddings.\n",
        "        patch_size (`int`): Patch size to turn the input data into small patches.\n",
        "        in_channels (`int`, *optional*, defaults to 16): The number of channels in the input.\n",
        "        num_layers (`int`, *optional*, defaults to 18): The number of layers of Transformer blocks to use.\n",
        "        attention_head_dim (`int`, *optional*, defaults to 64): The number of channels in each head.\n",
        "        num_attention_heads (`int`, *optional*, defaults to 18): The number of heads to use for multi-head attention.\n",
        "        cross_attention_dim (`int`, *optional*): The number of `encoder_hidden_states` dimensions to use.\n",
        "        caption_projection_dim (`int`): Number of dimensions to use when projecting the `encoder_hidden_states`.\n",
        "        pooled_projection_dim (`int`): Number of dimensions to use when projecting the `pooled_projections`.\n",
        "        out_channels (`int`, defaults to 16): Number of output channels.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _supports_gradient_checkpointing = True\n",
        "\n",
        "    @register_to_config\n",
        "    def __init__(\n",
        "        self,\n",
        "        sample_size: int = 128,\n",
        "        patch_size: int = 2,\n",
        "        in_channels: int = 16,\n",
        "        num_layers: int = 18,\n",
        "        attention_head_dim: int = 64,\n",
        "        num_attention_heads: int = 18,\n",
        "        joint_attention_dim: int = 4096,\n",
        "        caption_projection_dim: int = 1152,\n",
        "        pooled_projection_dim: int = 2048,\n",
        "        out_channels: int = 16,\n",
        "        pos_embed_max_size: int = 96,\n",
        "    ):\n",
        "        print(\"CUSTOM TRANSFORMER\")\n",
        "        print(\"sample_size\", sample_size)\n",
        "        super().__init__(\n",
        "            sample_size=sample_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            num_layers=num_layers,\n",
        "            attention_head_dim=attention_head_dim,\n",
        "            num_attention_heads=num_attention_heads,\n",
        "            joint_attention_dim=joint_attention_dim,\n",
        "            caption_projection_dim=caption_projection_dim,\n",
        "            pooled_projection_dim=pooled_projection_dim,\n",
        "            out_channels=out_channels,\n",
        "            pos_embed_max_size=pos_embed_max_size\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.FloatTensor,\n",
        "        encoder_hidden_states: torch.FloatTensor = None,\n",
        "        pooled_projections: torch.FloatTensor = None,\n",
        "        timestep: torch.LongTensor = None,\n",
        "        block_controlnet_hidden_states: List = None,\n",
        "        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        return_dict: bool = True,\n",
        "    ) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n",
        "        \"\"\"\n",
        "        The [`SD3Transformer2DModel`] forward method.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor` of shape `(batch size, channel, height, width)`):\n",
        "                Input `hidden_states`.\n",
        "            encoder_hidden_states (`torch.FloatTensor` of shape `(batch size, sequence_len, embed_dims)`):\n",
        "                Conditional embeddings (embeddings computed from the input conditions such as prompts) to use.\n",
        "            pooled_projections (`torch.FloatTensor` of shape `(batch_size, projection_dim)`): Embeddings projected\n",
        "                from the embeddings of input conditions.\n",
        "            timestep ( `torch.LongTensor`):\n",
        "                Used to indicate denoising step.\n",
        "            block_controlnet_hidden_states: (`list` of `torch.Tensor`):\n",
        "                A list of tensors that if specified are added to the residuals of transformer blocks.\n",
        "            joint_attention_kwargs (`dict`, *optional*):\n",
        "                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
        "                `self.processor` in\n",
        "                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n",
        "            return_dict (`bool`, *optional*, defaults to `True`):\n",
        "                Whether or not to return a [`~models.transformer_2d.Transformer2DModelOutput`] instead of a plain\n",
        "                tuple.\n",
        "\n",
        "        Returns:\n",
        "            If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`] is returned, otherwise a\n",
        "            `tuple` where the first element is the sample tensor.\n",
        "        \"\"\"\n",
        "        \n",
        "        print(\"custom transformer forward\")\n",
        "        if joint_attention_kwargs is not None:\n",
        "            joint_attention_kwargs = joint_attention_kwargs.copy()\n",
        "            lora_scale = joint_attention_kwargs.pop(\"scale\", 1.0)\n",
        "        else:\n",
        "            lora_scale = 1.0\n",
        "\n",
        "        if USE_PEFT_BACKEND:\n",
        "            # weight the lora layers by setting `lora_scale` for each PEFT layer\n",
        "            scale_lora_layers(self, lora_scale)\n",
        "        else:\n",
        "            if joint_attention_kwargs is not None and joint_attention_kwargs.get(\"scale\", None) is not None:\n",
        "                logger.warning(\n",
        "                    \"Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.\"\n",
        "                )\n",
        "\n",
        "        height, width = hidden_states.shape[-2:]\n",
        "\n",
        "        hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.\n",
        "        temb = self.time_text_embed(timestep, pooled_projections)\n",
        "        encoder_hidden_states = self.context_embedder(encoder_hidden_states)\n",
        "\n",
        "        for index_block, block in enumerate(self.transformer_blocks):\n",
        "            if self.training and self.gradient_checkpointing:\n",
        "\n",
        "                def create_custom_forward(module, return_dict=None):\n",
        "                    def custom_forward(*inputs):\n",
        "                        if return_dict is not None:\n",
        "                            return module(*inputs, return_dict=return_dict)\n",
        "                        else:\n",
        "                            return module(*inputs)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                ckpt_kwargs: Dict[str, Any] = {\"use_reentrant\": False} if is_torch_version(\">=\", \"1.11.0\") else {}\n",
        "                encoder_hidden_states, hidden_states = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(block),\n",
        "                    hidden_states,\n",
        "                    encoder_hidden_states,\n",
        "                    temb,\n",
        "                    **ckpt_kwargs,\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                encoder_hidden_states, hidden_states = block(\n",
        "                    hidden_states=hidden_states, encoder_hidden_states=encoder_hidden_states, temb=temb\n",
        "                )\n",
        "\n",
        "            # controlnet residual\n",
        "            if block_controlnet_hidden_states is not None and block.context_pre_only is False:\n",
        "                interval_control = len(self.transformer_blocks) // len(block_controlnet_hidden_states)\n",
        "                hidden_states = hidden_states + block_controlnet_hidden_states[index_block // interval_control]\n",
        "\n",
        "        hidden_states = self.norm_out(hidden_states, temb)\n",
        "        hidden_states = self.proj_out(hidden_states)\n",
        "        \n",
        "        print(\"hidden_states.shape\", hidden_states.shape)\n",
        "        #print(\"encoder_hidden_states\", encoder_hidden_states)\n",
        "\n",
        "        # unpatchify\n",
        "        patch_size = self.config.patch_size\n",
        "        height = height // patch_size\n",
        "        width = width // patch_size\n",
        "\n",
        "        hidden_states = hidden_states.reshape(\n",
        "            shape=(hidden_states.shape[0], height, width, patch_size, patch_size, self.out_channels)\n",
        "        )\n",
        "        hidden_states = torch.einsum(\"nhwpqc->nchpwq\", hidden_states)\n",
        "        output = hidden_states.reshape(\n",
        "            shape=(hidden_states.shape[0], self.out_channels, height * patch_size, width * patch_size)\n",
        "        )\n",
        "\n",
        "        print(\"output.shape\", output.shape)\n",
        "        \n",
        "        if USE_PEFT_BACKEND:\n",
        "            # remove `lora_scale` from each PEFT layer\n",
        "            unscale_lora_layers(self, lora_scale)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (output,)\n",
        "\n",
        "        return Transformer2DModelOutput(sample=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "1d94a2fa-00b0-45a2-bbb9-8e2072788b95",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    CLIPTextModelWithProjection,\n",
        "    CLIPTokenizer,\n",
        "    T5EncoderModel,\n",
        "    T5TokenizerFast,\n",
        ")\n",
        "\n",
        "from diffusers.image_processor import VaeImageProcessor\n",
        "from diffusers.loaders import FromSingleFileMixin, SD3LoraLoaderMixin\n",
        "from diffusers.models.autoencoders import AutoencoderKL\n",
        "from diffusers.models.transformers import SD3Transformer2DModel\n",
        "from diffusers import StableDiffusion3Pipeline\n",
        "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
        "from diffusers.utils import (\n",
        "    USE_PEFT_BACKEND,\n",
        "    is_torch_xla_available,\n",
        "    logging,\n",
        "    replace_example_docstring,\n",
        "    scale_lora_layers,\n",
        "    unscale_lora_layers,\n",
        ")\n",
        "from diffusers.utils.torch_utils import randn_tensor\n",
        "from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n",
        "from diffusers.pipelines.stable_diffusion_3.pipeline_output import StableDiffusion3PipelineOutput\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "if is_torch_xla_available():\n",
        "    import torch_xla.core.xla_model as xm\n",
        "\n",
        "    XLA_AVAILABLE = True\n",
        "else:\n",
        "    XLA_AVAILABLE = False\n",
        "\n",
        "\n",
        "EXAMPLE_DOC_STRING = \"\"\"\n",
        "    Examples:\n",
        "        ```py\n",
        "        >>> import torch\n",
        "        >>> from diffusers import StableDiffusion3Pipeline\n",
        "\n",
        "        >>> pipe = StableDiffusion3Pipeline.from_pretrained(\n",
        "        ...     \"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16\n",
        "        ... )\n",
        "        >>> pipe.to(\"cuda\")\n",
        "        >>> prompt = \"A cat holding a sign that says hello world\"\n",
        "        >>> image = pipe(prompt).images[0]\n",
        "        >>> image.save(\"sd3.png\")\n",
        "        ```\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from diffusers import StableDiffusion3Pipeline\n",
        "from copy import deepcopy\n",
        "\n",
        "def copy_weights(custom_module, original_module):\n",
        "    for name, custom_child in custom_module.named_children():\n",
        "        if hasattr(original_module, name):\n",
        "            original_child = getattr(original_module, name)\n",
        "            if isinstance(custom_child, nn.Linear) and isinstance(original_child, nn.Linear):\n",
        "                custom_child.weight.data.copy_(original_child.weight.data)\n",
        "                if custom_child.bias is not None:\n",
        "                    custom_child.bias.data.copy_(original_child.bias.data)\n",
        "            elif isinstance(custom_child, nn.LayerNorm) and isinstance(original_child, nn.LayerNorm):\n",
        "                if custom_child.elementwise_affine and original_child.elementwise_affine:\n",
        "                    custom_child.weight.data.copy_(original_child.weight.data)\n",
        "                    custom_child.bias.data.copy_(original_child.bias.data)\n",
        "            elif isinstance(custom_child, nn.Module) and isinstance(original_child, nn.Module):\n",
        "                copy_weights(custom_child, original_child)\n",
        "                \n",
        "                \n",
        "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n",
        "def retrieve_timesteps(\n",
        "    scheduler,\n",
        "    num_inference_steps: Optional[int] = None,\n",
        "    device: Optional[Union[str, torch.device]] = None,\n",
        "    timesteps: Optional[List[int]] = None,\n",
        "    sigmas: Optional[List[float]] = None,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
        "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
        "\n",
        "    Args:\n",
        "        scheduler (`SchedulerMixin`):\n",
        "            The scheduler to get timesteps from.\n",
        "        num_inference_steps (`int`):\n",
        "            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n",
        "            must be `None`.\n",
        "        device (`str` or `torch.device`, *optional*):\n",
        "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
        "        timesteps (`List[int]`, *optional*):\n",
        "            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n",
        "            `num_inference_steps` and `sigmas` must be `None`.\n",
        "        sigmas (`List[float]`, *optional*):\n",
        "            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n",
        "            `num_inference_steps` and `timesteps` must be `None`.\n",
        "\n",
        "    Returns:\n",
        "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
        "        second element is the number of inference steps.\n",
        "    \"\"\"\n",
        "    if timesteps is not None and sigmas is not None:\n",
        "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
        "    if timesteps is not None:\n",
        "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
        "        if not accepts_timesteps:\n",
        "            raise ValueError(\n",
        "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
        "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
        "            )\n",
        "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    elif sigmas is not None:\n",
        "        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
        "        if not accept_sigmas:\n",
        "            raise ValueError(\n",
        "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
        "                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n",
        "            )\n",
        "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "        num_inference_steps = len(timesteps)\n",
        "    else:\n",
        "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
        "        timesteps = scheduler.timesteps\n",
        "    return timesteps, num_inference_steps\n",
        "\n",
        "\n",
        "class CustomStableDiffusion3Pipeline(StableDiffusion3Pipeline):\n",
        "    r\"\"\"\n",
        "    Args:\n",
        "        transformer ([`CustomSD3Transformer2DModel`]):\n",
        "            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n",
        "        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n",
        "            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n",
        "        vae ([`AutoencoderKL`]):\n",
        "            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
        "        text_encoder ([`CLIPTextModelWithProjection`]):\n",
        "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),\n",
        "            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant,\n",
        "            with an additional added projection layer that is initialized with a diagonal matrix with the `hidden_size`\n",
        "            as its dimension.\n",
        "        text_encoder_2 ([`CLIPTextModelWithProjection`]):\n",
        "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),\n",
        "            specifically the\n",
        "            [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)\n",
        "            variant.\n",
        "        text_encoder_3 ([`T5EncoderModel`]):\n",
        "            Frozen text-encoder. Stable Diffusion 3 uses\n",
        "            [T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel), specifically the\n",
        "            [t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl) variant.\n",
        "        tokenizer (`CLIPTokenizer`):\n",
        "            Tokenizer of class\n",
        "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
        "        tokenizer_2 (`CLIPTokenizer`):\n",
        "            Second Tokenizer of class\n",
        "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
        "        tokenizer_3 (`T5TokenizerFast`):\n",
        "            Tokenizer of class\n",
        "            [T5Tokenizer](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer).\n",
        "    \"\"\"\n",
        "\n",
        "    model_cpu_offload_seq = \"text_encoder->text_encoder_2->text_encoder_3->transformer->vae\"\n",
        "    _optional_components = []\n",
        "    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\", \"negative_pooled_prompt_embeds\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        transformer: CustomSD3Transformer2DModel,\n",
        "        scheduler: FlowMatchEulerDiscreteScheduler,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModelWithProjection,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        text_encoder_2: CLIPTextModelWithProjection,\n",
        "        tokenizer_2: CLIPTokenizer,\n",
        "        text_encoder_3: T5EncoderModel,\n",
        "        tokenizer_3: T5TokenizerFast,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            transformer=transformer,\n",
        "            scheduler=scheduler,\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            text_encoder_2=text_encoder_2,\n",
        "            tokenizer_2=tokenizer_2,\n",
        "            text_encoder_3=text_encoder_3,\n",
        "            tokenizer_3=tokenizer_3\n",
        "        )\n",
        "\n",
        "        \"\"\"self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            text_encoder_2=text_encoder_2,\n",
        "            text_encoder_3=text_encoder_3,\n",
        "            tokenizer=tokenizer,\n",
        "            tokenizer_2=tokenizer_2,\n",
        "            tokenizer_3=tokenizer_3,\n",
        "            transformer=transformer,\n",
        "            scheduler=scheduler,\n",
        "        )\n",
        "        self.vae_scale_factor = (\n",
        "            2 ** (len(self.vae.config.block_out_channels) - 1) if hasattr(self, \"vae\") and self.vae is not None else 8\n",
        "        )\n",
        "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
        "        self.tokenizer_max_length = (\n",
        "            self.tokenizer.model_max_length if hasattr(self, \"tokenizer\") and self.tokenizer is not None else 77\n",
        "        )\n",
        "        self.default_sample_size = (\n",
        "            self.transformer.config.sample_size\n",
        "            if hasattr(self, \"transformer\") and self.transformer is not None\n",
        "            else 128\n",
        "        )\n",
        "        self.patch_size = (\n",
        "            self.transformer.config.patch_size if hasattr(self, \"transformer\") and self.transformer is not None else 2\n",
        "        )\"\"\"\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        prompt_3: Optional[Union[str, List[str]]] = None,\n",
        "        height: Optional[int] = None,\n",
        "        width: Optional[int] = None,\n",
        "        num_inference_steps: int = 28,\n",
        "        timesteps: List[int] = None,\n",
        "        guidance_scale: float = 7.0,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        negative_prompt_3: Optional[Union[str, List[str]]] = None,\n",
        "        num_images_per_prompt: Optional[int] = 1,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        clip_skip: Optional[int] = None,\n",
        "        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n",
        "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
        "        max_sequence_length: int = 256,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Function invoked when calling the pipeline for generation.\n",
        "\n",
        "        Args:\n",
        "            prompt (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
        "                instead.\n",
        "            prompt_2 (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n",
        "                will be used instead\n",
        "            prompt_3 (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts to be sent to `tokenizer_3` and `text_encoder_3`. If not defined, `prompt` is\n",
        "                will be used instead\n",
        "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
        "                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n",
        "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
        "                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n",
        "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
        "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
        "                expense of slower inference.\n",
        "            timesteps (`List[int]`, *optional*):\n",
        "                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n",
        "                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n",
        "                passed will be used. Must be in descending order.\n",
        "            guidance_scale (`float`, *optional*, defaults to 7.0):\n",
        "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
        "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
        "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
        "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
        "                usually at the expense of lower image quality.\n",
        "            negative_prompt (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
        "                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
        "                less than `1`).\n",
        "            negative_prompt_2 (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and\n",
        "                `text_encoder_2`. If not defined, `negative_prompt` is used instead\n",
        "            negative_prompt_3 (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts not to guide the image generation to be sent to `tokenizer_3` and\n",
        "                `text_encoder_3`. If not defined, `negative_prompt` is used instead\n",
        "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
        "                The number of images to generate per prompt.\n",
        "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
        "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
        "                to make generation deterministic.\n",
        "            latents (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
        "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
        "                tensor will ge generated by sampling using the supplied random `generator`.\n",
        "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
        "                provided, text embeddings will be generated from `prompt` input argument.\n",
        "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
        "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
        "                argument.\n",
        "            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n",
        "                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n",
        "            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
        "                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`\n",
        "                input argument.\n",
        "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
        "                The output format of the generate image. Choose between\n",
        "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
        "            return_dict (`bool`, *optional*, defaults to `True`):\n",
        "                Whether or not to return a [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] instead\n",
        "                of a plain tuple.\n",
        "            joint_attention_kwargs (`dict`, *optional*):\n",
        "                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
        "                `self.processor` in\n",
        "                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n",
        "            callback_on_step_end (`Callable`, *optional*):\n",
        "                A function that calls at the end of each denoising steps during the inference. The function is called\n",
        "                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n",
        "                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n",
        "                `callback_on_step_end_tensor_inputs`.\n",
        "            callback_on_step_end_tensor_inputs (`List`, *optional*):\n",
        "                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n",
        "                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n",
        "                `._callback_tensor_inputs` attribute of your pipeline class.\n",
        "            max_sequence_length (`int` defaults to 256): Maximum sequence length to use with the `prompt`.\n",
        "\n",
        "        Examples:\n",
        "\n",
        "        Returns:\n",
        "            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] or `tuple`:\n",
        "            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] if `return_dict` is True, otherwise a\n",
        "            `tuple`. When returning a tuple, the first element is a list with the generated images.\n",
        "        \"\"\"\n",
        "        \n",
        "        print(\"in the func\")\n",
        "        height = height or self.default_sample_size * self.vae_scale_factor\n",
        "        width = width or self.default_sample_size * self.vae_scale_factor\n",
        "\n",
        "        # 1. Check inputs. Raise error if not correct\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            prompt_2,\n",
        "            prompt_3,\n",
        "            height,\n",
        "            width,\n",
        "            negative_prompt=negative_prompt,\n",
        "            negative_prompt_2=negative_prompt_2,\n",
        "            negative_prompt_3=negative_prompt_3,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
        "            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n",
        "            max_sequence_length=max_sequence_length,\n",
        "        )\n",
        "\n",
        "        self._guidance_scale = guidance_scale\n",
        "        self._clip_skip = clip_skip\n",
        "        self._joint_attention_kwargs = joint_attention_kwargs\n",
        "        self._interrupt = False\n",
        "\n",
        "        # 2. Define call parameters\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "\n",
        "        lora_scale = (\n",
        "            self.joint_attention_kwargs.get(\"scale\", None) if self.joint_attention_kwargs is not None else None\n",
        "        )\n",
        "        (\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "        ) = self.encode_prompt(\n",
        "            prompt=prompt,\n",
        "            prompt_2=prompt_2,\n",
        "            prompt_3=prompt_3,\n",
        "            negative_prompt=negative_prompt,\n",
        "            negative_prompt_2=negative_prompt_2,\n",
        "            negative_prompt_3=negative_prompt_3,\n",
        "            do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
        "            device=device,\n",
        "            clip_skip=self.clip_skip,\n",
        "            num_images_per_prompt=num_images_per_prompt,\n",
        "            max_sequence_length=max_sequence_length,\n",
        "            lora_scale=lora_scale,\n",
        "        )\n",
        "        \n",
        "        print(\"prompt_embeds\", prompt_embeds)\n",
        "\n",
        "        if self.do_classifier_free_guidance:\n",
        "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
        "            pooled_prompt_embeds = torch.cat([negative_pooled_prompt_embeds, pooled_prompt_embeds], dim=0)\n",
        "\n",
        "        # 4. Prepare timesteps\n",
        "        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)\n",
        "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
        "        self._num_timesteps = len(timesteps)\n",
        "\n",
        "        # 5. Prepare latent variables\n",
        "        num_channels_latents = self.transformer.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_images_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            width,\n",
        "            prompt_embeds.dtype,\n",
        "            device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "\n",
        "        print(\"latents.shape\", latents.shape)\n",
        "        \n",
        "        # 6. Denoising loop\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                if self.interrupt:\n",
        "                    continue\n",
        "\n",
        "                # expand the latents if we are doing classifier free guidance\n",
        "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
        "                print(\"latent_model_input.shape\", latent_model_input.shape)\n",
        "                \n",
        "                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "                timestep = t.expand(latent_model_input.shape[0])\n",
        "\n",
        "                noise_pred = self.transformer(\n",
        "                    hidden_states=latent_model_input,\n",
        "                    timestep=timestep,\n",
        "                    encoder_hidden_states=prompt_embeds,\n",
        "                    pooled_projections=pooled_prompt_embeds,\n",
        "                    joint_attention_kwargs=self.joint_attention_kwargs,\n",
        "                    return_dict=False,\n",
        "                )[0]\n",
        "                \n",
        "                #print(\"noise_pred\", noise_pred)\n",
        "\n",
        "                # perform guidance\n",
        "                if self.do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                # compute the previous noisy sample x_t -> x_t-1\n",
        "                latents_dtype = latents.dtype\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
        "                print(\"latents.shape\", latents.shape)\n",
        "                \n",
        "                #MADE A CHANGE FOR SCIENCE\n",
        "                #latents[:,:6,:,:] = 0.1\n",
        "                \n",
        "                if latents.dtype != latents_dtype:\n",
        "                    if torch.backends.mps.is_available():\n",
        "                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
        "                        latents = latents.to(latents_dtype)\n",
        "\n",
        "                if callback_on_step_end is not None:\n",
        "                    callback_kwargs = {}\n",
        "                    for k in callback_on_step_end_tensor_inputs:\n",
        "                        callback_kwargs[k] = locals()[k]\n",
        "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
        "\n",
        "                    latents = callback_outputs.pop(\"latents\", latents)\n",
        "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
        "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
        "                    negative_pooled_prompt_embeds = callback_outputs.pop(\n",
        "                        \"negative_pooled_prompt_embeds\", negative_pooled_prompt_embeds\n",
        "                    )\n",
        "\n",
        "                # call the callback, if provided\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "\n",
        "                if XLA_AVAILABLE:\n",
        "                    xm.mark_step()\n",
        "                    \n",
        "                temp_image = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor\n",
        "                temp_image = self.vae.decode(temp_image, return_dict=False)[0]\n",
        "                temp_image = self.image_processor.postprocess(temp_image, output_type=output_type)\n",
        "                print(\"temp_image\", temp_image)\n",
        "                display(temp_image[0])\n",
        "                del temp_image\n",
        "\n",
        "        if output_type == \"latent\":\n",
        "            image = latents\n",
        "\n",
        "        else:\n",
        "            latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor\n",
        "\n",
        "            image = self.vae.decode(latents, return_dict=False)[0]\n",
        "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
        "\n",
        "        # Offload all models\n",
        "        self.maybe_free_model_hooks()\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image,)\n",
        "\n",
        "        return StableDiffusion3PipelineOutput(images=image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca8a756b-a660-478e-b920-c35a5499d91e",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusion3Pipeline\n",
        "import sentencepiece\n",
        "\n",
        "pipe = CustomStableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", \n",
        "    text_encoder_3=None,\n",
        "    tokenizer_3=None,\n",
        "    torch_dtype=torch.float16)\n",
        "\n",
        "\n",
        "original_transformer = pipe.transformer\n",
        "\n",
        "# Create our custom transformer\n",
        "custom_transformer = CustomSD3Transformer2DModel(sample_size=original_transformer.config.sample_size,\n",
        "    patch_size=original_transformer.config.patch_size,\n",
        "    in_channels=original_transformer.config.in_channels,\n",
        "    num_layers=original_transformer.config.num_layers,\n",
        "    attention_head_dim=original_transformer.config.attention_head_dim,\n",
        "    num_attention_heads=original_transformer.config.num_attention_heads,\n",
        "    joint_attention_dim=original_transformer.config.joint_attention_dim,\n",
        "    caption_projection_dim=original_transformer.config.caption_projection_dim,\n",
        "    pooled_projection_dim=original_transformer.config.pooled_projection_dim,\n",
        "    out_channels=original_transformer.config.out_channels,\n",
        "    pos_embed_max_size=original_transformer.config.pos_embed_max_size)\n",
        "\n",
        "# Copy weights from the original transformer to our custom transformer\n",
        "with torch.no_grad():\n",
        "    custom_transformer.pos_embed = deepcopy(original_transformer.pos_embed)\n",
        "    custom_transformer.time_text_embed = deepcopy(original_transformer.time_text_embed)\n",
        "    custom_transformer.context_embedder = deepcopy(original_transformer.context_embedder)\n",
        "    custom_transformer.transformer_blocks = deepcopy(original_transformer.transformer_blocks)\n",
        "    custom_transformer.norm_out = deepcopy(original_transformer.norm_out)\n",
        "    custom_transformer.proj_out = deepcopy(original_transformer.proj_out)\n",
        "    copy_weights(custom_transformer, original_transformer)\n",
        "\n",
        "# Replace the transformer in the UNet with our custom transformer\n",
        "pipe.transformer = custom_transformer\n",
        "\n",
        "pipe.enable_model_cpu_offload()\n",
        "#pipe = pipe.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9505a2-831c-4bae-91ea-dcccb7f6de04",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "image = pipe(\n",
        "    \"A cat\",\n",
        "    negative_prompt=\"\",\n",
        "    num_inference_steps=10,\n",
        "    guidance_scale=7.0,\n",
        "    height=1024,\n",
        "    width=1024,\n",
        ").images[0]\n",
        "\n",
        "image\n",
        "\n",
        "image = pipe(\n",
        "    \"A city skyline\",\n",
        "    negative_prompt=\"\",\n",
        "    num_inference_steps=10,\n",
        "    guidance_scale=7.0,\n",
        "    height=1024,\n",
        "    width=1024,\n",
        ").images[0]\n",
        "\n",
        "image\n",
        "\n",
        "\n",
        "image = pipe(\n",
        "    \"10 pineapples\",\n",
        "    negative_prompt=\"\",\n",
        "    num_inference_steps=10,\n",
        "    guidance_scale=7.0,\n",
        "    height=1024,\n",
        "    width=1024,\n",
        ").images[0]\n",
        "\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8 - Pytorch and Tensorflow",
      "language": "python",
      "name": "python38-azureml-pt-tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
