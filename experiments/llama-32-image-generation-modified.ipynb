{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1495edef-3e31-4f51-bf8b-fad59046019b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (4.46.0.dev0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface_hub in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (0.25.2)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.26.0) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (4.46.0.dev0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install numpy\n",
    "%pip install huggingface_hub\n",
    "%pip install 'accelerate>=0.26.0'\n",
    "%pip install --upgrade transformers\n",
    "\n",
    "from huggingface_hub import login\n",
    "import local_config\n",
    "\n",
    "login(token=local_config.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6886bb3c-b9c1-4b9b-a89a-3472bf9aff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb479a7-341e-4448-b2a0-536e2593f242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (0.30.3)\n",
      "Requirement already satisfied: importlib-metadata in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (8.4.0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (0.25.2)\n",
      "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (0.4.5)\n",
      "Requirement already satisfied: Pillow in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (10.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895c185e-44b0-4330-9998-cc2fb1afd3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (4.46.0.dev0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d66e50e-8b3a-43c6-a4ef-e93346474a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nfrom transformers import LlamaModel, LlamaConfig, MllamaProcessor, MllamaVisionModel\\nfrom transformers import AutoProcessor, AutoModelForPreTraining\\nimport numpy as np\\nimport os\\nfrom huggingface_hub import HfApi, HfFolder\\nfrom huggingface_hub.utils import RepositoryNotFoundError, RevisionNotFoundError\\n\\n\\nHfFolder.save_token(local_config.TOKEN)\\n\\n# Create a directory to save the model\\n#os.makedirs(\"downloaded_model\", exist_ok=True)\\n\\nprocessor = MllamaProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\", token=local_config.TOKEN)\\n#model = MllamaVisionModel.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\", token=local_config.TOKEN)\\n\\ncheckpoint = \"meta-llama/Llama-3.2-11B-Vision\"\\nmodel = MllamaVisionModel.from_pretrained(checkpoint, token=local_config.TOKEN)\\nprint(\"model\", model)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "from transformers import LlamaModel, LlamaConfig, MllamaProcessor, MllamaVisionModel\n",
    "from transformers import AutoProcessor, AutoModelForPreTraining\n",
    "import numpy as np\n",
    "import os\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from huggingface_hub.utils import RepositoryNotFoundError, RevisionNotFoundError\n",
    "\n",
    "\n",
    "HfFolder.save_token(local_config.TOKEN)\n",
    "\n",
    "# Create a directory to save the model\n",
    "#os.makedirs(\"downloaded_model\", exist_ok=True)\n",
    "\n",
    "processor = MllamaProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\", token=local_config.TOKEN)\n",
    "#model = MllamaVisionModel.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\", token=local_config.TOKEN)\n",
    "\n",
    "checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "model = MllamaVisionModel.from_pretrained(checkpoint, token=local_config.TOKEN)\n",
    "print(\"model\", model)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e928a2-255a-4a15-9abc-5bacf8933faa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-nder38rn\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-nder38rn\n",
      "  Resolved https://github.com/huggingface/transformers to commit fa3f2db5c7405a742fcb8f686d3754f70db00977\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2024.9.11)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers==4.46.0.dev0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7460126-0076-45f7-917e-9ebb7b57de79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec8cde2-84d5-48c1-8c60-773b6c614cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.0.dev0\n",
      "4.46.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:24:45.164115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-14 10:24:45.186855: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-14 10:24:45.193290: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-14 10:24:45.208896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 10:24:46.382265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)\n",
    "\n",
    "import torch\n",
    "print(transformers.__version__)\n",
    "from transformers import MllamaForConditionalGeneration\n",
    "from transformers import LlamaModel, MllamaConfig, MllamaProcessor, MllamaVisionModel\n",
    "from transformers import AutoProcessor, AutoModelForPreTraining, AutoConfig\n",
    "import numpy as np\n",
    "import os\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from huggingface_hub.utils import RepositoryNotFoundError, RevisionNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff79626-c8f5-4eb4-bcf0-08b7f1262a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union, Tuple\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import MllamaVisionModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class CustomMllamaForConditionalGeneration(MllamaForConditionalGeneration):\n",
    "    def __init__(self, config: MllamaConfig):\n",
    "        super().__init__(config)\n",
    "        self.vocab_size = config.text_config.vocab_size\n",
    "        self.hidden_size = config.text_config.hidden_size\n",
    "        self.max_num_tiles = config.vision_config.max_num_tiles\n",
    "        self.vision_output_dim = config.vision_config.vision_output_dim\n",
    "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
    "\n",
    "        self.vision_model = MllamaVisionModel._from_config(\n",
    "            config.vision_config, attn_implementation=config._attn_implementation\n",
    "        )\n",
    "        self.vision_model2 = MllamaVisionModel._from_config(\n",
    "            config.vision_config, attn_implementation=config._attn_implementation\n",
    "        )\n",
    "        \n",
    "        self.vision_model2.transformer = self.vision_model.transformer\n",
    "        self.vision_model2.global_transformer = self.vision_model.global_transformer\n",
    "        self.vision_model2.patch_embedding = self.vision_model.patch_embedding\n",
    "        self.vision_model2.gated_positional_embedding = self.vision_model.gated_positional_embedding\n",
    "        self.vision_model2.pre_tile_positional_embedding = self.vision_model.pre_tile_positional_embedding\n",
    "        self.vision_model2.post_tile_positional_embedding = self.vision_model.post_tile_positional_embedding\n",
    "        self.vision_model2.layernorm_pre = self.vision_model.layernorm_pre\n",
    "        self.vision_model2.layernorm_post = self.vision_model.layernorm_post\n",
    "        self.vision_model2.class_embedding = self.vision_model.class_embedding\n",
    "\n",
    "        self.language_model = MllamaForCausalLM._from_config(\n",
    "            config.text_config, attn_implementation=config._attn_implementation\n",
    "        )\n",
    "        self.multi_modal_projector = nn.Linear(\n",
    "            config.vision_config.vision_output_dim,\n",
    "            config.text_config.hidden_size,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        aspect_ratio_mask: Optional[torch.Tensor] = None,\n",
    "        aspect_ratio_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attention_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attention_states: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        num_logits_to_keep: int = 0,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "            num_logits_to_keep (`int`, *optional*):\n",
    "                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n",
    "                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n",
    "                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "\n",
    "        >>> checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "        >>> model = MllamaForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        >>> processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "        >>> prompt = \"<|image|>If I had to write a haiku for this one\"\n",
    "        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> output = model.generate(**inputs, max_new_tokens=15)\n",
    "\n",
    "        >>> prompt_len = inputs.input_ids.shape[-1]\n",
    "        >>> generated_ids = output[:, prompt_len:]\n",
    "        >>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        >>> print(generated_text)\n",
    "        [', it would be:.\\\\nA stop sign in Chinatown.\\\\n']\n",
    "        ```\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if pixel_values is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if pixel_values is not None and cross_attention_states is not None:\n",
    "            raise ValueError(\"`pixel_values` and `cross_attention_states` cannot be provided simultaneously\")\n",
    "        \n",
    "        if pixel_values is not None:\n",
    "            #print(\"self.vision_model2.transformer.layers[0].self_attn.q_proj.weight\", self.vision_model2.transformer.layers[0].self_attn.q_proj.weight)\n",
    "            if aspect_ratio_ids is None:\n",
    "                raise ValueError(\"`aspect_ratio_ids` must be provided if `pixel_values` is provided\")\n",
    "            # get vision tokens from vision model\n",
    "            vision_outputs2 = self.vision_model2(\n",
    "                pixel_values=pixel_values,\n",
    "                aspect_ratio_ids=aspect_ratio_ids,\n",
    "                aspect_ratio_mask=aspect_ratio_mask,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                output_attentions=output_attentions,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            cross_attention_states2 = vision_outputs2[0]\n",
    "            cross_attention_states2 = self.multi_modal_projector(cross_attention_states2).reshape(\n",
    "                -1, cross_attention_states2.shape[-2], self.hidden_size\n",
    "            )\n",
    "            #print(\"cross_attention_states2\", cross_attention_states2)\n",
    "            \n",
    "        if pixel_values is not None:\n",
    "            #print(\"self.vision_model.transformer.layers[0].self_attn.q_proj.weight\", self.vision_model.transformer.layers[0].self_attn.q_proj.weight)\n",
    "            if aspect_ratio_ids is None:\n",
    "                raise ValueError(\"`aspect_ratio_ids` must be provided if `pixel_values` is provided\")\n",
    "            # get vision tokens from vision model\n",
    "            vision_outputs = self.vision_model(\n",
    "                pixel_values=pixel_values,\n",
    "                aspect_ratio_ids=aspect_ratio_ids,\n",
    "                aspect_ratio_mask=aspect_ratio_mask,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                output_attentions=output_attentions,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            cross_attention_states = vision_outputs[0]\n",
    "            cross_attention_states = self.multi_modal_projector(cross_attention_states).reshape(\n",
    "                -1, cross_attention_states.shape[-2], self.hidden_size\n",
    "            )\n",
    "            #print(\"cross_attention_states\", cross_attention_states)\n",
    "\n",
    "            \n",
    "        \n",
    "        if cross_attention_states is not None:\n",
    "            cross_attention_states = (cross_attention_states + cross_attention_states2) / 2\n",
    "\n",
    "        if cross_attention_mask is not None:\n",
    "            cross_attention_mask, full_text_row_masked_out_mask = _prepare_cross_attention_mask(\n",
    "                cross_attention_mask,\n",
    "                num_vision_tokens=self.vision_model.num_patches,\n",
    "                dtype=self.dtype,\n",
    "            )\n",
    "        else:\n",
    "            full_text_row_masked_out_mask = None\n",
    "\n",
    "        if cross_attention_mask is not None and cache_position is not None:\n",
    "            cross_attention_mask = cross_attention_mask[:, :, cache_position]\n",
    "            full_text_row_masked_out_mask = full_text_row_masked_out_mask[:, :, cache_position]\n",
    "\n",
    "        outputs = self.language_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            cross_attention_states=cross_attention_states,\n",
    "            cross_attention_mask=cross_attention_mask,\n",
    "            full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            num_logits_to_keep=num_logits_to_keep,\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cec1c6ec-375f-49cd-ab36-491eacdcdf5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [01:57<00:00, 23.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(transformers.__version__)\n",
    "from transformers import MllamaForConditionalGeneration\n",
    "from transformers import LlamaModel, MllamaConfig, MllamaProcessor, MllamaVisionModel\n",
    "from transformers import AutoProcessor, AutoModelForPreTraining, AutoConfig\n",
    "import numpy as np\n",
    "import os\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from huggingface_hub.utils import RepositoryNotFoundError, RevisionNotFoundError\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForCausalLM, MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-11B-Vision\"  # You can change this to any model you have access to\n",
    "model = CustomMllamaForConditionalGeneration.from_pretrained(model_name, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/wes2/code\", token=local_config.TOKEN)\n",
    "\n",
    "#unaltered_model = MllamaForConditionalGeneration.from_pretrained(model_name, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/wes2/code\", token=local_config.TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b4f1295-7c48-4d4b-bcdc-36e9a2c2dec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _prepare_cross_attention_mask(\n",
    "    cross_attention_mask: torch.Tensor,\n",
    "    num_vision_tokens: int,\n",
    "    dtype: str,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # reshape so it can be used by attn module\n",
    "    batch_size, text_total_length, *_ = cross_attention_mask.shape\n",
    "    cross_attention_mask = cross_attention_mask.repeat_interleave(num_vision_tokens, dim=3)\n",
    "    cross_attention_mask = cross_attention_mask.view(batch_size, text_total_length, -1)\n",
    "    cross_attention_mask = cross_attention_mask.unsqueeze(1)\n",
    "\n",
    "    # invert the mask\n",
    "    inverted_cross_attn_mask = (1.0 - cross_attention_mask).to(dtype)\n",
    "    cross_attention_mask = inverted_cross_attn_mask.masked_fill(\n",
    "        inverted_cross_attn_mask.to(torch.bool), torch.finfo(dtype).min\n",
    "    )\n",
    "\n",
    "    # apply full-row bias, which return 4D tensor of shape [B, H, S1, 1] where value is 0 if the a full row in cross attn mask's\n",
    "    # last dimension contains negative infinity values, otherwise it's 1\n",
    "    negative_inf_value = torch.finfo(dtype).min\n",
    "    full_text_row_masked_out_mask = (\n",
    "        (cross_attention_mask != negative_inf_value).any(dim=-1).type_as(cross_attention_mask)[..., None]\n",
    "    )\n",
    "    cross_attention_mask *= full_text_row_masked_out_mask\n",
    "\n",
    "    return cross_attention_mask, full_text_row_masked_out_mask\n",
    "\n",
    "\n",
    "def _prepare_aspect_ratio_attention_mask(\n",
    "    aspect_ratio_mask: torch.Tensor,\n",
    "    num_patches: int,\n",
    "    target_length: int,\n",
    "    dtype: torch.dtype,\n",
    ") -> torch.Tensor:\n",
    "    # Expand aspect ratio mask to target_length\n",
    "    batch_size, max_num_tiles = aspect_ratio_mask.shape\n",
    "    attention_mask = aspect_ratio_mask.view(batch_size, max_num_tiles, 1, 1).to(dtype)\n",
    "    attention_mask = attention_mask.repeat(1, 1, target_length, 1)\n",
    "\n",
    "    # Mask padding patches\n",
    "    pad_patches = target_length - num_patches\n",
    "    attention_mask[:, :, -pad_patches:] = 0\n",
    "\n",
    "    # Invert the mask (0 -> 1, 1 -> 0)\n",
    "    attention_mask = 1 - attention_mask\n",
    "\n",
    "    # Reshape to 2D and create 4D attention mask\n",
    "    # (batch_size, 1, max_num_tiles * target_length, max_num_tiles * target_length)\n",
    "    attention_mask = attention_mask.reshape(batch_size, max_num_tiles * target_length, 1)\n",
    "    attention_mask = attention_mask @ attention_mask.transpose(-1, -2) * torch.finfo(dtype).min\n",
    "    attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "    return attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad38aac-34e7-41c6-8a62-ad521d611f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "\n",
    "\n",
    "# Load the original configuration\n",
    "original_config = MllamaConfig.from_pretrained(model_name, cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/wes2/code\", token=local_config.TOKEN)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3833bf9c-bf97-4433-b1dd-57991f82a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|image|><|begin_of_text|>If I had to write a haiku for this one, it would be:.\\nA dog in a top hat.\\nA dog in a top hat.\\nA dog in a top hat.\\\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "url = \"https://c8.alamy.com/comp/E9J0EW/dog-with-hat-E9J0EW.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "prompt = \"<|image|><|begin_of_text|>If I had to write a haiku for this one\"\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "#output = unaltered_model.generate(**inputs, max_new_tokens=30)\n",
    "#print(processor.decode(output[0]))\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=30)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e6b5e3-14da-4575-8237-829b6c5ef461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (0.30.3)\n",
      "Requirement already satisfied: transformers in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (4.46.0.dev0)\n",
      "Requirement already satisfied: scipy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: torch in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: accelerate in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (8.4.0)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (0.25.2)\n",
      "Requirement already satisfied: numpy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (0.4.5)\n",
      "Requirement already satisfied: Pillow in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from diffusers) (10.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: psutil in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.25.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->diffusers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade diffusers transformers scipy torch accelerate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d94a2fa-00b0-45a2-bbb9-8e2072788b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPTokenizer,\n",
    "    T5EncoderModel,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.loaders import FromSingleFileMixin, SD3LoraLoaderMixin\n",
    "from diffusers.models.autoencoders import AutoencoderKL\n",
    "from diffusers.models.transformers import SD3Transformer2DModel\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
    "from diffusers.utils import (\n",
    "    USE_PEFT_BACKEND,\n",
    "    is_torch_xla_available,\n",
    "    logging,\n",
    "    replace_example_docstring,\n",
    "    scale_lora_layers,\n",
    "    unscale_lora_layers,\n",
    ")\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion_3.pipeline_output import StableDiffusion3PipelineOutput\n",
    "\n",
    "\n",
    "\n",
    "if is_torch_xla_available():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "\n",
    "    XLA_AVAILABLE = True\n",
    "else:\n",
    "    XLA_AVAILABLE = False\n",
    "\n",
    "\n",
    "EXAMPLE_DOC_STRING = \"\"\"\n",
    "    Examples:\n",
    "        ```py\n",
    "        >>> import torch\n",
    "        >>> from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "        >>> pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "        ...     \"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16\n",
    "        ... )\n",
    "        >>> pipe.to(\"cuda\")\n",
    "        >>> prompt = \"A cat holding a sign that says hello world\"\n",
    "        >>> image = pipe(prompt).images[0]\n",
    "        >>> image.save(\"sd3.png\")\n",
    "        ```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    sigmas: Optional[List[float]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n",
    "            must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n",
    "            `num_inference_steps` and `sigmas` must be `None`.\n",
    "        sigmas (`List[float]`, *optional*):\n",
    "            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n",
    "            `num_inference_steps` and `timesteps` must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None and sigmas is not None:\n",
    "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    elif sigmas is not None:\n",
    "        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accept_sigmas:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "\n",
    "class CustomStableDiffusion3Pipeline(StableDiffusion3Pipeline):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        transformer ([`SD3Transformer2DModel`]):\n",
    "            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n",
    "        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n",
    "            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n",
    "        vae ([`AutoencoderKL`]):\n",
    "            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
    "        text_encoder ([`CLIPTextModelWithProjection`]):\n",
    "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),\n",
    "            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant,\n",
    "            with an additional added projection layer that is initialized with a diagonal matrix with the `hidden_size`\n",
    "            as its dimension.\n",
    "        text_encoder_2 ([`CLIPTextModelWithProjection`]):\n",
    "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),\n",
    "            specifically the\n",
    "            [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)\n",
    "            variant.\n",
    "        text_encoder_3 ([`T5EncoderModel`]):\n",
    "            Frozen text-encoder. Stable Diffusion 3 uses\n",
    "            [T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel), specifically the\n",
    "            [t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl) variant.\n",
    "        tokenizer (`CLIPTokenizer`):\n",
    "            Tokenizer of class\n",
    "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
    "        tokenizer_2 (`CLIPTokenizer`):\n",
    "            Second Tokenizer of class\n",
    "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
    "        tokenizer_3 (`T5TokenizerFast`):\n",
    "            Tokenizer of class\n",
    "            [T5Tokenizer](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer).\n",
    "    \"\"\"\n",
    "\n",
    "    model_cpu_offload_seq = \"text_encoder->text_encoder_2->text_encoder_3->transformer->vae\"\n",
    "    _optional_components = []\n",
    "    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\", \"negative_pooled_prompt_embeds\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer: SD3Transformer2DModel,\n",
    "        scheduler: FlowMatchEulerDiscreteScheduler,\n",
    "        vae: AutoencoderKL,\n",
    "        text_encoder: CLIPTextModelWithProjection,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        text_encoder_2: CLIPTextModelWithProjection,\n",
    "        tokenizer_2: CLIPTokenizer,\n",
    "        text_encoder_3: T5EncoderModel,\n",
    "        tokenizer_3: T5TokenizerFast,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            transformer=transformer,\n",
    "            scheduler=scheduler,\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            text_encoder_2=text_encoder_2,\n",
    "            tokenizer_2=tokenizer_2,\n",
    "            text_encoder_3=text_encoder_3,\n",
    "            tokenizer_3=tokenizer_3\n",
    "        )\n",
    "\n",
    "        \"\"\"self.register_modules(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            text_encoder_2=text_encoder_2,\n",
    "            text_encoder_3=text_encoder_3,\n",
    "            tokenizer=tokenizer,\n",
    "            tokenizer_2=tokenizer_2,\n",
    "            tokenizer_3=tokenizer_3,\n",
    "            transformer=transformer,\n",
    "            scheduler=scheduler,\n",
    "        )\n",
    "        self.vae_scale_factor = (\n",
    "            2 ** (len(self.vae.config.block_out_channels) - 1) if hasattr(self, \"vae\") and self.vae is not None else 8\n",
    "        )\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
    "        self.tokenizer_max_length = (\n",
    "            self.tokenizer.model_max_length if hasattr(self, \"tokenizer\") and self.tokenizer is not None else 77\n",
    "        )\n",
    "        self.default_sample_size = (\n",
    "            self.transformer.config.sample_size\n",
    "            if hasattr(self, \"transformer\") and self.transformer is not None\n",
    "            else 128\n",
    "        )\n",
    "        self.patch_size = (\n",
    "            self.transformer.config.patch_size if hasattr(self, \"transformer\") and self.transformer is not None else 2\n",
    "        )\"\"\"\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        prompt_3: Optional[Union[str, List[str]]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 28,\n",
    "        timesteps: List[int] = None,\n",
    "        guidance_scale: float = 7.0,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "        negative_prompt_3: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        max_sequence_length: int = 256,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
    "                instead.\n",
    "            prompt_2 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n",
    "                will be used instead\n",
    "            prompt_3 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to be sent to `tokenizer_3` and `text_encoder_3`. If not defined, `prompt` is\n",
    "                will be used instead\n",
    "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n",
    "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n",
    "                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n",
    "                passed will be used. Must be in descending order.\n",
    "            guidance_scale (`float`, *optional*, defaults to 7.0):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
    "                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
    "                less than `1`).\n",
    "            negative_prompt_2 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and\n",
    "                `text_encoder_2`. If not defined, `negative_prompt` is used instead\n",
    "            negative_prompt_3 (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation to be sent to `tokenizer_3` and\n",
    "                `text_encoder_3`. If not defined, `negative_prompt` is used instead\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
    "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
    "                to make generation deterministic.\n",
    "            latents (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
    "                argument.\n",
    "            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n",
    "                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n",
    "            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`\n",
    "                input argument.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] instead\n",
    "                of a plain tuple.\n",
    "            joint_attention_kwargs (`dict`, *optional*):\n",
    "                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
    "                `self.processor` in\n",
    "                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n",
    "            callback_on_step_end (`Callable`, *optional*):\n",
    "                A function that calls at the end of each denoising steps during the inference. The function is called\n",
    "                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n",
    "                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n",
    "                `callback_on_step_end_tensor_inputs`.\n",
    "            callback_on_step_end_tensor_inputs (`List`, *optional*):\n",
    "                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n",
    "                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n",
    "                `._callback_tensor_inputs` attribute of your pipeline class.\n",
    "            max_sequence_length (`int` defaults to 256): Maximum sequence length to use with the `prompt`.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] or `tuple`:\n",
    "            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] if `return_dict` is True, otherwise a\n",
    "            `tuple`. When returning a tuple, the first element is a list with the generated images.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"in the func\")\n",
    "        height = height or self.default_sample_size * self.vae_scale_factor\n",
    "        width = width or self.default_sample_size * self.vae_scale_factor\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            prompt_2,\n",
    "            prompt_3,\n",
    "            height,\n",
    "            width,\n",
    "            negative_prompt=negative_prompt,\n",
    "            negative_prompt_2=negative_prompt_2,\n",
    "            negative_prompt_3=negative_prompt_3,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "        )\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._joint_attention_kwargs = joint_attention_kwargs\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        lora_scale = (\n",
    "            self.joint_attention_kwargs.get(\"scale\", None) if self.joint_attention_kwargs is not None else None\n",
    "        )\n",
    "        (\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds,\n",
    "        ) = self.encode_prompt(\n",
    "            prompt=prompt,\n",
    "            prompt_2=prompt_2,\n",
    "            prompt_3=prompt_3,\n",
    "            negative_prompt=negative_prompt,\n",
    "            negative_prompt_2=negative_prompt_2,\n",
    "            negative_prompt_3=negative_prompt_3,\n",
    "            do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            device=device,\n",
    "            clip_skip=self.clip_skip,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            lora_scale=lora_scale,\n",
    "        )\n",
    "\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            pooled_prompt_embeds = torch.cat([negative_pooled_prompt_embeds, pooled_prompt_embeds], dim=0)\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "        self._num_timesteps = len(timesteps)\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = self.transformer.config.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Denoising loop\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "                timestep = t.expand(latent_model_input.shape[0])\n",
    "\n",
    "                noise_pred = self.transformer(\n",
    "                    hidden_states=latent_model_input,\n",
    "                    timestep=timestep,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    pooled_projections=pooled_prompt_embeds,\n",
    "                    joint_attention_kwargs=self.joint_attention_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents_dtype = latents.dtype\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "                if latents.dtype != latents_dtype:\n",
    "                    if torch.backends.mps.is_available():\n",
    "                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                        latents = latents.to(latents_dtype)\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
    "                    negative_pooled_prompt_embeds = callback_outputs.pop(\n",
    "                        \"negative_pooled_prompt_embeds\", negative_pooled_prompt_embeds\n",
    "                    )\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "\n",
    "                if XLA_AVAILABLE:\n",
    "                    xm.mark_step()\n",
    "\n",
    "        if output_type == \"latent\":\n",
    "            image = latents\n",
    "\n",
    "        else:\n",
    "            latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor\n",
    "\n",
    "            image = self.vae.decode(latents, return_dict=False)[0]\n",
    "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return StableDiffusion3PipelineOutput(images=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca8a756b-a660-478e-b920-c35a5499d91e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  2.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "import sentencepiece\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", \n",
    "    text_encoder_3=None,\n",
    "    tokenizer_3=None,\n",
    "    torch_dtype=torch.float16)\n",
    "pipe.enable_model_cpu_offload()\n",
    "#pipe = pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d9505a2-831c-4bae-91ea-dcccb7f6de04",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 47.12 MiB is free. Including non-PyTorch memory, this process has 15.72 GiB memory in use. Of the allocated memory 15.08 GiB is allocated by PyTorch, and 277.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA cat holding a picture of itself holding a picture of itself\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m image\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py:825\u001b[0m, in \u001b[0;36mStableDiffusion3Pipeline.__call__\u001b[0;34m(self, prompt, prompt_2, prompt_3, height, width, num_inference_steps, timesteps, guidance_scale, negative_prompt, negative_prompt_2, negative_prompt_3, num_images_per_prompt, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, joint_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length)\u001b[0m\n\u001b[1;32m    815\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execution_device\n\u001b[1;32m    817\u001b[0m lora_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_attention_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    819\u001b[0m )\n\u001b[1;32m    820\u001b[0m (\n\u001b[1;32m    821\u001b[0m     prompt_embeds,\n\u001b[1;32m    822\u001b[0m     negative_prompt_embeds,\n\u001b[1;32m    823\u001b[0m     pooled_prompt_embeds,\n\u001b[1;32m    824\u001b[0m     negative_pooled_prompt_embeds,\n\u001b[0;32m--> 825\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_3\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_3\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt_3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_pooled_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_pooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_classifier_free_guidance:\n\u001b[1;32m    845\u001b[0m     prompt_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([negative_prompt_embeds, prompt_embeds], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py:410\u001b[0m, in \u001b[0;36mStableDiffusion3Pipeline.encode_prompt\u001b[0;34m(self, prompt, prompt_2, prompt_3, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, negative_prompt_2, negative_prompt_3, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, clip_skip, max_sequence_length, lora_scale)\u001b[0m\n\u001b[1;32m    407\u001b[0m prompt_3 \u001b[38;5;241m=\u001b[39m prompt_3 \u001b[38;5;129;01mor\u001b[39;00m prompt\n\u001b[1;32m    408\u001b[0m prompt_3 \u001b[38;5;241m=\u001b[39m [prompt_3] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt_3, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m prompt_3\n\u001b[0;32m--> 410\u001b[0m prompt_embed, pooled_prompt_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_clip_prompt_embeds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_model_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m prompt_2_embed, pooled_prompt_2_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_clip_prompt_embeds(\n\u001b[1;32m    418\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt_2,\n\u001b[1;32m    419\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     clip_model_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    423\u001b[0m )\n\u001b[1;32m    424\u001b[0m clip_prompt_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prompt_embed, prompt_2_embed], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py:298\u001b[0m, in \u001b[0;36mStableDiffusion3Pipeline._get_clip_prompt_embeds\u001b[0;34m(self, prompt, num_images_per_prompt, device, clip_skip, clip_model_index)\u001b[0m\n\u001b[1;32m    293\u001b[0m     removed_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(untruncated_ids[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    294\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following part of your input was truncated because CLIP can only handle sequences up to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_max_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremoved_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m pooled_prompt_embeds \u001b[38;5;241m=\u001b[39m prompt_embeds[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_skip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/accelerate/hooks.py:702\u001b[0m, in \u001b[0;36mCpuOffload.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_module_hook\u001b[38;5;241m.\u001b[39moffload()\n\u001b[1;32m    701\u001b[0m     clear_device_cache()\n\u001b[0;32m--> 702\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/transformers/modeling_utils.py:3124\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3120\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3121\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3123\u001b[0m         )\n\u001b[0;32m-> 3124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 47.12 MiB is free. Including non-PyTorch memory, this process has 15.72 GiB memory in use. Of the allocated memory 15.08 GiB is allocated by PyTorch, and 277.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "image = pipe(\n",
    "    \"A cat holding a picture of itself holding a picture of itself\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=10,\n",
    "    guidance_scale=7.0,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    ").images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c62fb-2188-46a3-b079-71b48d643937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"pipe\", pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf93a78-23ea-47cb-bb16-84a5b5af97c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047738f3-7f73-45ee-977d-afa7528cc970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57bb0a-aff5-4d44-8c28-8ff135dc4395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f73e1a-3634-4de5-a2af-23a01327113b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855bc4b-58f2-401a-88aa-fe2d0a0ef174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
